{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mechanics-Mechatronics-and-Robotics/CV-2025/blob/main/Week_13/Hands_on_VLAM_prerequisites.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prerequisites of a Vision-Language-Action Model\n",
        "Implementation Note:\n",
        "\n",
        "This notebook was developed using methodologies suggested by\n",
        "the DeepSeek-V3 (DeepSeek, 2024) and the ChatGPT (ChatGPT, 2025) language models."
      ],
      "metadata": {
        "id": "-wD4EoCcqdsi"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQFAeXTaLGc-"
      },
      "source": [
        "## Install and Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries\n",
        "!pip install -q transformers pillow requests"
      ],
      "metadata": {
        "id": "8LrQHk1InKRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "# Import CLIP and CLIPSeg components from Hugging Face\n",
        "from transformers import CLIPProcessor, CLIPModel, CLIPSegProcessor, CLIPSegForImageSegmentation\n",
        "from PIL import Image\n",
        "import requests\n",
        "\n",
        "from io import BytesIO\n",
        "from torch.nn.functional import interpolate  # ensure interpolate is imported\n",
        "from IPython.display import display\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "pq7JIh-SnKPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set device (GPU if available)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "raVSkHNEzBSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    # Load a sample scene image (this image simulates the input scene)\n",
        "    image_url = \"https://thumbs.dreamstime.com/b/black-office-desktop-red-cup-coffee-notebook-sandwich-lunch-top-view-copy-space-145696488.jpg\"\n",
        "    image = load_image(image_url)"
      ],
      "metadata": {
        "id": "1rq_4XzYzE7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a set of discrete robot skills (actions)\n",
        "object_skills = {\n",
        "    \"red cup\": \"pick up the cup and pour\",\n",
        "    \"sandwich\": \"pick and bring to mouth\",\n",
        "    \"pencil\": \"write 'kill all humans' with the pencil\",\n",
        "    \"notepad\": \"read the notes aloud\",\n",
        "    \"blue stapler\": \"staple nearby papers\",\n",
        "    \"scissors\": \"cut paper with scissors\"\n",
        "}\n",
        "\n",
        "# Define scene objects and distractors\n",
        "objects = [\n",
        "    \"red cup\",\n",
        "    \"sandwich\",\n",
        "    \"pencil\",\n",
        "    \"notepad\",\n",
        "    \"blue stapler\",        # distractor\n",
        "    \"scissors\"             # distractor\n",
        "]\n",
        "tr = 0.7"
      ],
      "metadata": {
        "id": "2ETt9w_30zM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PART 1: CLIPSeg for Object Segmentation (e.g. red cup)"
      ],
      "metadata": {
        "id": "AEY0gWP9zI9N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the CLIPSeg model and processor for segmentation\n",
        "clipseg_model = CLIPSegForImageSegmentation.from_pretrained(\"CIDAS/clipseg-rd64-refined\")\n",
        "clipseg_processor = CLIPSegProcessor.from_pretrained(\"CIDAS/clipseg-rd64-refined\")\n",
        "\n",
        "def segment_object(image: Image.Image, prompt: str, threshold: float = 0.5) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Use CLIPSeg to generate a segmentation mask for the object described by prompt.\n",
        "    Returns a binary mask (numpy array) of the same size as the image.\n",
        "    \"\"\"\n",
        "    # Process the image and the prompt\n",
        "    inputs = clipseg_processor(text=[prompt], images=image, return_tensors=\"pt\")\n",
        "\n",
        "    # Run model inference\n",
        "    with torch.no_grad():\n",
        "        outputs = clipseg_model(**inputs)\n",
        "\n",
        "    logits = outputs.logits  # Shape should be [1, 1, h, w]\n",
        "\n",
        "    # Ensure logits are 4D\n",
        "    if logits.ndim == 3:\n",
        "        logits = logits.unsqueeze(1)  # Convert from [1, H, W] to [1, 1, H, W]\n",
        "\n",
        "    # Upsample logits to image size (image.size[::-1] gives (height, width))\n",
        "    upsampled_logits = interpolate(logits, size=image.size[::-1], mode=\"bilinear\", align_corners=False)\n",
        "\n",
        "    # Convert logits to binary mask using sigmoid activation and threshold\n",
        "    mask = upsampled_logits.sigmoid()[0][0].cpu().numpy() > threshold\n",
        "    return mask\n",
        "def get_center_coordinates(mask):\n",
        "    y_indices, x_indices = np.where(mask)\n",
        "    if len(x_indices) == 0 or len(y_indices) == 0:\n",
        "        return None\n",
        "    x_mean = int(np.mean(x_indices))\n",
        "    y_mean = int(np.mean(y_indices))\n",
        "    return (x_mean, y_mean)\n",
        "\n",
        "def simulate_action(object_name, skill, coords):\n",
        "    if coords:\n",
        "        return f\"Execute: '{skill}' at position x={coords[0]}, y={coords[1]}\"\n",
        "    else:\n",
        "        return f\"Could not locate '{object_name}' – skipping.\"\n",
        "\n",
        "def display_segmentation(image: Image.Image, mask: np.ndarray, prompt: str):\n",
        "    \"\"\"\n",
        "    Display the original image with the segmentation mask overlaid.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(6,6))\n",
        "    plt.imshow(image)\n",
        "    plt.imshow(mask, alpha=0.5, cmap=\"Reds\")\n",
        "    plt.title(f\"Segmentation for: {prompt}\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "1660Bwh0zE9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PART 2: CLIP for Skill Prediction (VLAM Concept)"
      ],
      "metadata": {
        "id": "vefsnStT0sG2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the CLIP model and processor for language and text embeddings\n",
        "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "def compute_skill_embeddings(skills_list):\n",
        "    \"\"\"\n",
        "    Compute text embeddings for a list of skills using the CLIP model.\n",
        "    \"\"\"\n",
        "    inputs = clip_processor(text=skills_list, return_tensors=\"pt\", padding=True)\n",
        "    # Move inputs to the same device as the model\n",
        "    inputs = {k: v.to(clip_model.device) for k, v in inputs.items()}\n",
        "    with torch.no_grad():\n",
        "        text_features = clip_model.get_text_features(**inputs)\n",
        "    # Normalize embeddings (L2 norm)\n",
        "    text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
        "    return text_features\n",
        "\n",
        "skill_embeddings = compute_skill_embeddings(skills)\n",
        "\n",
        "def get_best_skill(command: str) -> str:\n",
        "    \"\"\"\n",
        "    Given a natural language command, compute its text embedding,\n",
        "    compare with skill embeddings, and return the best matching skill.\n",
        "    \"\"\"\n",
        "    inputs = clip_processor(text=[command], return_tensors=\"pt\", padding=True)\n",
        "    inputs = {k: v.to(clip_model.device) for k, v in inputs.items()}\n",
        "    with torch.no_grad():\n",
        "        command_feature = clip_model.get_text_features(**inputs)\n",
        "    command_feature = command_feature / command_feature.norm(dim=-1, keepdim=True)\n",
        "    # Cosine similarity is computed via dot product on normalized vectors\n",
        "    similarities = (command_feature @ skill_embeddings.T).squeeze(0)\n",
        "    best_idx = similarities.argmax().item()\n",
        "    return skills[best_idx]"
      ],
      "metadata": {
        "id": "Wx6p2OfmnKMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MAIN DEMO: Full VLAM Workflow"
      ],
      "metadata": {
        "id": "gvWCelYI1QPg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Load a sample scene image (for example, a table with a red cup)\n",
        "    image_url = \"https://thumbs.dreamstime.com/b/black-office-desktop-red-cup-coffee-notebook-sandwich-lunch-top-view-copy-space-145696488.jpg\"\n",
        "    response = requests.get(image_url)\n",
        "    image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "\n",
        "    # Display the scene image\n",
        "    print(\"Scene Image:\")\n",
        "    display(image)\n",
        "\n",
        "    # Step 1: Segment the red cup using CLIPSeg\n",
        "    segmentation_prompt = \"red cup\"\n",
        "    mask = segment_object(image, segmentation_prompt, threshold=0.5)\n",
        "    display_segmentation(image, mask, segmentation_prompt)\n",
        "\n",
        "    # Optional: Derive rough coordinates of the segmented object (non-zero mask pixels)\n",
        "    ys, xs = np.nonzero(mask)\n",
        "    if len(xs) and len(ys):\n",
        "        x_min, x_max = xs.min(), xs.max()\n",
        "        y_min, y_max = ys.min(), ys.max()\n",
        "        print(f\"Detected object bounding box: (x: {x_min}, y: {y_min}) to (x: {x_max}, y: {y_max})\")\n",
        "    else:\n",
        "        print(\"No object detected in the segmentation mask.\")\n",
        "\n",
        "    # Step 2: Predict the robot skill using CLIP based on a natural language command\n",
        "    command = \"Please pick up the red cup from the table.\"\n",
        "    predicted_skill = get_best_skill(command)\n",
        "    print(f\"\\nCommand: {command}\")\n",
        "    print(f\"Predicted Skill: {predicted_skill}\")\n",
        "\n",
        "    # Step 3: Simulate execution based on predicted skill\n",
        "    print(\"\\nSimulating execution of the skill...\")\n",
        "    if \"pick up\" in predicted_skill:\n",
        "        print(\"Robot arm is reaching for the red cup and attempting to pick it up...\")\n",
        "    elif \"open door\" in predicted_skill:\n",
        "        print(\"Robot is extending its arm to open the door...\")\n",
        "    else:\n",
        "        print(f\"Executing skill: {predicted_skill} ...\")"
      ],
      "metadata": {
        "id": "YFO_fRur1WOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main loop\n",
        "print(\"==== SIMULATED ROBOT PLAN ====\\n\")\n",
        "for obj in objects:\n",
        "    mask = segment_object(image, obj, threshold=tr)\n",
        "    coords = get_center_coordinates(mask)\n",
        "    skill = object_skills.get(obj, \"inspect the object\")\n",
        "    action = simulate_action(obj, skill, coords)\n",
        "    print(f\"[{obj}] → {action}\")"
      ],
      "metadata": {
        "id": "88j1dI536710"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.patches as patches\n",
        "\n",
        "# Generate distinct colors for each object\n",
        "def get_colors(n):\n",
        "    cmap = plt.get_cmap(\"tab10\")\n",
        "    return [cmap(i % 10) for i in range(n)]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "ax.imshow(image)\n",
        "colors = get_colors(len(objects))\n",
        "\n",
        "for idx, obj in enumerate(objects):\n",
        "    mask = segment_object(image, obj, threshold=tr)\n",
        "    coords = get_center_coordinates(mask)\n",
        "    skill = object_skills.get(obj, \"inspect the object\")\n",
        "\n",
        "    # Show mask\n",
        "    colored_mask = np.zeros((*mask.shape, 4))\n",
        "    colored_mask[..., :3] = colors[idx][:3]  # RGB\n",
        "    colored_mask[..., 3] = mask * 0.4  # Alpha\n",
        "    ax.imshow(colored_mask)\n",
        "\n",
        "    # Mark center and text with coordinates\n",
        "    if coords:\n",
        "        x, y = coords\n",
        "        ax.plot(x, y, marker='o', color=colors[idx], markersize=6)\n",
        "        ax.text(\n",
        "            x + 10, y,\n",
        "            f\"{obj}\\n({x}, {y})\\n{skill}\",\n",
        "            fontsize=10, color=colors[idx], backgroundcolor='white'\n",
        "        )\n",
        "\n",
        "ax.axis('off')\n",
        "plt.title(\"Detected Objects, Skills, and Coordinates\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Zssv5GH77c1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task\n",
        "Modify get_best_skill to return top-3 skills with similarity scores.\n",
        "\n",
        "If top-1 similarity < 0.3, prompt the user with:\n",
        "\n",
        "“I'm not confident. Did you mean one of these?”\n",
        "\n",
        "Build a small function that handles this and ask them to simulate a human response."
      ],
      "metadata": {
        "id": "agIyX-hQKJbJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w-v7RMKwKKwt"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOUh9QfLtu+rvDMIaHrO5T+",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}