{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mechanics-Mechatronics-and-Robotics/CV-2025/blob/main/Week_13/Lab10_CLIP_fine_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 10. CLIP fine-tuning."
      ],
      "metadata": {
        "id": "LrrvkMq6uFCI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/openai/CLIP.git"
      ],
      "metadata": {
        "id": "1mmn2azSz4Jq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let's recall what is fine-tuning itself?**\n",
        "\n",
        "In machine learning, fine-tuning is a process of taking a pre-trained model and “tuning” its parameters slightly to adapt to a new, similar task.\n",
        "\n",
        "**Why do we need this?**\n",
        "\n",
        "* Save resources\n",
        "* Transfer learning effect\n",
        "* Limited data\n",
        "\n",
        "To complete this lab we will fine-tune a model entity from Hugging Face repo.\n",
        "\n",
        "Let's start with importing libraries..."
      ],
      "metadata": {
        "id": "J8pcBNnLuCGg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6Q9ffe4nkgp"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "import glob\n",
        "import os\n",
        "import pandas as pd\n",
        "import json\n",
        "import numpy as np\n",
        "import clip\n",
        "from torch.utils.data import Dataset, DataLoader, BatchSampler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm.notebook import tqdm\n",
        "import random\n",
        "from matplotlib.pyplot import imshow\n",
        "\n",
        "import nltk, re, string, collections\n",
        "from nltk.util import ngrams\n",
        "import collections\n",
        "\n",
        "%matplotlib inline\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "EPOCH = 5"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1WizPT7uYnpsEEelztKt3aLbRoFCzuijj"
      ],
      "metadata": {
        "id": "nlMi-HNul_nN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip memes.zip"
      ],
      "metadata": {
        "id": "-b3nMJU3maZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_ROOT = \"mems_images\"\n",
        "JSON_ROOT = \"meme_project_clean_json\"\n",
        "img_paths = glob.glob(os.path.join(IMG_ROOT, \"*.jpg\"))\n",
        "d = {}\n",
        "for i, img_path in enumerate(img_paths):\n",
        "    name = img_path.split(\"/\")[-1].split(\".\")[0]\n",
        "    with open(os.path.join(JSON_ROOT, name+\".json\"), \"r\") as f:\n",
        "        captions = json.load(f)\n",
        "        temp = []\n",
        "        for cap in captions:\n",
        "            if \"http\" not in (cap[0]+ ' '+cap[1]) and len(cap[0]+ ' '+cap[1]) >= 8 and len(cap[0]+ ' '+cap[1]) <= 72:\n",
        "                temp.append(cap[0]+ ' '+cap[1])\n",
        "        d[img_path] = temp\n",
        "len(d)"
      ],
      "metadata": {
        "id": "4dwHGVmhx2_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Image.open(list(d.items())[0][0])"
      ],
      "metadata": {
        "id": "3P2uh0qpvh_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(d.items())[0]"
      ],
      "metadata": {
        "id": "VXdMbai1twL3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splitting our dataset..."
      ],
      "metadata": {
        "id": "fM2N8XEsv3s3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_img_paths, test_img_paths = train_test_split(img_paths, test_size=0.2, random_state=42)\n",
        "d_train = {k: d[k] for k in train_img_paths}\n",
        "d_test = {k: d[k] for k in test_img_paths}\n",
        "len(d_train), len(d_test)"
      ],
      "metadata": {
        "id": "rwh0p5n3u4nB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloading our model"
      ],
      "metadata": {
        "id": "3DE8XYF2wENQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)"
      ],
      "metadata": {
        "id": "-d75_Jxpv7t_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MemeDataset(Dataset):\n",
        "    def __init__(self, data, preprocess):\n",
        "        self.preprocess = preprocess\n",
        "        self.img_paths = []\n",
        "        self.captions = []\n",
        "        for img_path, captions in data.items():\n",
        "            for cap in captions:\n",
        "                self.img_paths.append(img_path)\n",
        "                self.captions.append(cap)\n",
        "        self.processed_cache = {}\n",
        "        for img_path in data:\n",
        "            self.processed_cache[img_path] = self.preprocess(Image.open(img_path))\n",
        "        self.img_paths_set = list(data.keys())\n",
        "        self.path2label = {path: self.img_paths_set.index(path) for path in self.img_paths_set}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.captions)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.img_paths[idx]\n",
        "        image = self.processed_cache[img_path]\n",
        "        caption = self.captions[idx]\n",
        "        label = self.path2label[img_path]\n",
        "        return image, caption, label\n",
        "\n",
        "train_dataset = MemeDataset(d_train, preprocess)\n",
        "test_dataset = MemeDataset(d_test, preprocess)\n"
      ],
      "metadata": {
        "id": "r50UxM41wDbk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BalancedBatchSampler(BatchSampler):\n",
        "    \"\"\"\n",
        "    BatchSampler - from a MNIST-like dataset, samples n_classes and within these classes samples n_samples.\n",
        "    Returns batches of size n_classes * n_samples\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, labels, n_classes, n_samples):\n",
        "        self.labels = labels\n",
        "        self.labels_set = list(set(self.labels.numpy()))\n",
        "        self.label_to_indices = {label: np.where(self.labels.numpy() == label)[0]\n",
        "                                 for label in self.labels_set}\n",
        "        for l in self.labels_set:\n",
        "            np.random.shuffle(self.label_to_indices[l])\n",
        "        self.used_label_indices_count = {label: 0 for label in self.labels_set}\n",
        "        self.count = 0\n",
        "        self.n_classes = n_classes\n",
        "        self.n_samples = n_samples\n",
        "        self.n_dataset = len(self.labels)\n",
        "        self.batch_size = self.n_samples * self.n_classes\n",
        "\n",
        "    def __iter__(self):\n",
        "        self.count = 0\n",
        "        while self.count + self.batch_size < self.n_dataset:\n",
        "            classes = np.random.choice(self.labels_set, self.n_classes, replace=False)\n",
        "            indices = []\n",
        "            for class_ in classes:\n",
        "                indices.extend(self.label_to_indices[class_][\n",
        "                               self.used_label_indices_count[class_]:self.used_label_indices_count[\n",
        "                                                                         class_] + self.n_samples])\n",
        "                self.used_label_indices_count[class_] += self.n_samples\n",
        "                if self.used_label_indices_count[class_] + self.n_samples > len(self.label_to_indices[class_]):\n",
        "                    np.random.shuffle(self.label_to_indices[class_])\n",
        "                    self.used_label_indices_count[class_] = 0\n",
        "            yield indices\n",
        "            self.count += self.n_classes * self.n_samples\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_dataset // self.batch_size\n",
        "\n",
        "train_labels = torch.tensor([item[2] for item in train_dataset])\n",
        "train_sampler = BalancedBatchSampler(train_labels, BATCH_SIZE, 1)\n",
        "train_dataloader = DataLoader(train_dataset, batch_sampler=train_sampler)\n",
        "\n",
        "test_labels = torch.tensor([item[2] for item in test_dataset])\n",
        "test_sampler = BalancedBatchSampler(test_labels, BATCH_SIZE, 1)\n",
        "test_dataloader = DataLoader(test_dataset, batch_sampler=test_sampler)"
      ],
      "metadata": {
        "id": "Ld6y5IgVxwTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_models_to_fp32(model):\n",
        "    for p in model.parameters():\n",
        "        p.data = p.data.float()\n",
        "        p.grad.data = p.grad.data.float()\n",
        "\n",
        "if device == \"cpu\":\n",
        "    model.float()\n",
        "\n",
        "loss_img = nn.CrossEntropyLoss()\n",
        "loss_txt = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, len(train_dataloader)*EPOCH)"
      ],
      "metadata": {
        "id": "cceW6f1H1T5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_te_loss = 1e5\n",
        "best_ep = -1\n",
        "for epoch in range(EPOCH):\n",
        "    print(f\"running epoch {epoch}, best test loss {best_te_loss} after epoch {best_ep}\")\n",
        "    step = 0\n",
        "    tr_loss = 0\n",
        "    model.train()\n",
        "    pbar = tqdm(train_dataloader, leave=False)\n",
        "    for batch in pbar:\n",
        "        step += 1\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        images, texts, _ = batch\n",
        "        images = images.to(device)\n",
        "        texts = clip.tokenize(texts).to(device)\n",
        "\n",
        "        logits_per_image, logits_per_text = model(images, texts)\n",
        "        ground_truth = torch.arange(BATCH_SIZE).to(device)\n",
        "\n",
        "        total_loss = (loss_img(logits_per_image,ground_truth) + loss_txt(logits_per_text,ground_truth))/2\n",
        "\n",
        "        total_loss.backward()\n",
        "        tr_loss += total_loss.item()\n",
        "        if device == \"cpu\":\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "        else:\n",
        "            convert_models_to_fp32(model)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            clip.model.convert_weights(model)\n",
        "        pbar.set_description(f\"train batchCE: {total_loss.item()}\", refresh=True)\n",
        "    tr_loss /= step\n",
        "\n",
        "    step = 0\n",
        "    te_loss = 0\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        test_pbar = tqdm(test_dataloader, leave=False)\n",
        "        for batch in test_pbar:\n",
        "            step += 1\n",
        "            images, texts, _ = batch\n",
        "            images = images.to(device)\n",
        "            texts = clip.tokenize(texts).to(device)\n",
        "            logits_per_image, logits_per_text = model(images, texts)\n",
        "            ground_truth = torch.arange(BATCH_SIZE).to(device)\n",
        "\n",
        "            total_loss = (loss_img(logits_per_image,ground_truth) + loss_txt(logits_per_text,ground_truth))/2\n",
        "            te_loss += total_loss.item()\n",
        "            test_pbar.set_description(f\"test batchCE: {total_loss.item()}\", refresh=True)\n",
        "        te_loss /= step\n",
        "\n",
        "    if te_loss < best_te_loss:\n",
        "        best_te_loss = te_loss\n",
        "        best_ep = epoch\n",
        "        torch.save(model.state_dict(), \"best_model.pt\")\n",
        "    print(f\"epoch {epoch}, tr_loss {tr_loss}, te_loss {te_loss}\")\n",
        "torch.save(model.state_dict(), \"last_model.pt\")"
      ],
      "metadata": {
        "id": "fhcLiyEh1YcF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1rC0lq-4_-F4lAHY0aSTvLtYnHohPcqbk"
      ],
      "metadata": {
        "id": "c2hOAckuwTXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's download already fine-tuned model with 5 epoches..."
      ],
      "metadata": {
        "id": "NDyXzBtlxolO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(\"last_model.pt\", map_location=device))\n",
        "NUM_NEG = 32\n",
        "NUM_TEST = 1000"
      ],
      "metadata": {
        "id": "Z8WaCM0EgHlC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_correct = 0\n",
        "for i in tqdm(range(NUM_TEST)):\n",
        "    empty = True\n",
        "    while empty:\n",
        "        img_path = random.choice(list(d_test.keys()))\n",
        "        image = preprocess(Image.open(img_path)).unsqueeze(0).to(device)\n",
        "        name = img_path.split('/')[-1].split('.')[0]\n",
        "        caps = d_test[img_path]\n",
        "        if len(caps) > 0:\n",
        "            pos_txt = random.choice(caps)\n",
        "        #         pos_txt = ' '.join(pos_txt)\n",
        "            empty = False\n",
        "#     print(pos_txt)\n",
        "    neg_i = 0\n",
        "    neg_txts = []\n",
        "    while neg_i < NUM_NEG:\n",
        "        img_path = random.choice(list(d_test.keys()))\n",
        "        neg_name = img_path.split('/')[-1].split('.')[0]\n",
        "        if neg_name == name:\n",
        "            continue\n",
        "        caps = d_test[img_path]\n",
        "        if len(caps) == 0:\n",
        "            continue\n",
        "        neg_txt = random.choice(caps)\n",
        "        if neg_txt in neg_txts:\n",
        "            continue\n",
        "        neg_txts.append(neg_txt)\n",
        "        neg_i += 1\n",
        "#     print(name)\n",
        "#     print(f\"Positive caption: {pos_txt}\")\n",
        "#     print(f\"Negative caption: {neg_txts}\")\n",
        "    text = clip.tokenize([pos_txt]+neg_txts).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        image_features = model.encode_image(image)\n",
        "        text_features = model.encode_text(text)\n",
        "\n",
        "        logits_per_image, logits_per_text = model(image, text)\n",
        "        probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
        "\n",
        "#     print(\"Label probs:\", probs)\n",
        "#     print(np.argmax(probs))\n",
        "    if np.argmax(probs) == 0:\n",
        "        n_correct +=1\n",
        "print(f\"Test precision {n_correct/NUM_TEST}\")"
      ],
      "metadata": {
        "id": "39m8b3Wd1gw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = []\n",
        "for txtlist in d_train.values():\n",
        "    corpus += txtlist\n",
        "len(corpus), corpus[0]"
      ],
      "metadata": {
        "id": "LGYaf4SJhpCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample1Caption(img_path, corpus, model, num_cand):\n",
        "    image = preprocess(Image.open(img_path)).unsqueeze(0).to(device)\n",
        "    i = 0\n",
        "    txts = []\n",
        "    while i < num_cand:\n",
        "        txt = random.choice(corpus)\n",
        "        if txt in txts:\n",
        "            continue\n",
        "        if len(txt.split())<5 or len(txt)>72:\n",
        "            continue\n",
        "        txts.append(txt)\n",
        "        i += 1\n",
        "    #     print(name)\n",
        "    #     print(f\"Positive caption: {pos_txt}\")\n",
        "    #     print(f\"Negative caption: {neg_txts}\")\n",
        "    text = clip.tokenize(txts).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits_per_image, logits_per_text = model(image, text)\n",
        "        probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
        "\n",
        "    #     print(\"Label probs:\", probs)\n",
        "    #     print(np.argmax(probs))\n",
        "    #     imshow(np.asarray(Image.open(img_path)))\n",
        "    return txts[np.argmax(probs)]"
      ],
      "metadata": {
        "id": "Ci7oVjB9hDbt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seen_path = random.choice(list(d_train.keys()))\n",
        "pred_cap_seen = sample1Caption(seen_path, corpus, model, 1000)\n",
        "gt_cap_seen = d_train[seen_path][:5]\n",
        "imshow(Image.open(seen_path))\n",
        "print(f\"Some ground truth captions for this seen image: {gt_cap_seen}\")\n",
        "print(f\"Caption sampled by fintuned CLIP for this seen image: {pred_cap_seen}\")"
      ],
      "metadata": {
        "id": "tm2ZbASRg0ro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unseen_path = random.choice(list(d_test.keys()))\n",
        "pred_cap_unseen = sample1Caption(unseen_path, corpus, model, 1000)\n",
        "imshow(Image.open(unseen_path))\n",
        "gt_cap_unseen = d_test[unseen_path][:5]\n",
        "print(f\"Some ground truth captions for this unseen image: {gt_cap_unseen}\")\n",
        "print(f\"Caption sampled by fintuned CLIP for this unseen image: {pred_cap_unseen}\")"
      ],
      "metadata": {
        "id": "qf8wlll1hBfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Homework (2 points)"
      ],
      "metadata": {
        "id": "V83xMN9ZALD5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You have to choose architecture with text-to-image principle and fine-tune it to your task.\n",
        "\n",
        "Requirements:\n",
        "\n",
        "* Find an architecture, if CLIP - it should be a different dataset\n",
        "* Find a dataset adapted for text-to-image task\n",
        "* Prepare a dataset to fine-tune your model accordingly\n",
        "* Show metrics of fine-tune procedure (metrics for test set of something else)\n",
        "* Generate an image based on the text or text based on the image (think about a function for that)\n",
        "* You can arrange you own fine-tuning process\n",
        "\n",
        "Example of datasets:\n",
        "\n",
        "https://www.kaggle.com/datasets/hsankesara/flickr-image-dataset\n",
        "\n",
        "https://www.kaggle.com/datasets/validmodel/indo-fashion-dataset/data\n",
        "\n",
        "https://huggingface.co/datasets?task_categories=task_categories:text-to-image&sort=trending\n",
        "\n"
      ],
      "metadata": {
        "id": "1raEbBTiAOXv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hoOkbUP8AJHA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}