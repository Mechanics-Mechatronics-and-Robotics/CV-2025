{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMvzskFeP6VpswYdTyrUjDk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mechanics-Mechatronics-and-Robotics/CV-2025/blob/main/Week_14/Hands_on_CoT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hands-on Chains of Thoughts\n",
        "Implementation Note:\n",
        "\n",
        "This notebook was developed using methodologies suggested by the DeepSeek-V3 (DeepSeek, 2024) language model."
      ],
      "metadata": {
        "id": "2cP_Fo9Koxqj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 1: Install Dependencies\n",
        "!pip install -q torch transformers accelerate"
      ],
      "metadata": {
        "id": "wrEC18EoK359"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "import re"
      ],
      "metadata": {
        "id": "c6nLGZ3uRfJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Test Cases\n",
        "problems = [\n",
        "    \"A pizza is cut into 8 slices. If 3 people share equally, how many slices per person?\",\n",
        "    \"Буквы а и б сидели на трубе. А упала, б пропала, какая буква осталась на трубе?\",\n",
        "    \"A car travels 240 km in 3 hours. What's its speed in km/h?\"\n",
        "]"
      ],
      "metadata": {
        "id": "EYmJ1iahQuxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Load Quantized Model\n",
        "# Using Microsoft's Phi-2 (2.7B params but fits in T4 GPU with quantization)\n",
        "model_name = \"microsoft/phi-2\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16,\n",
        "    trust_remote_code=True\n",
        ")"
      ],
      "metadata": {
        "id": "b88BPwAuGetX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = next(model.parameters()).device  # e.g. cuda:0"
      ],
      "metadata": {
        "id": "vYzsVkYDu6yi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Direct Answer Function\n",
        "def direct_answer(prompt: str) -> str:\n",
        "    template = (\n",
        "        prompt.strip()\n",
        "        + \"\\n\\nAnswer only with the final value (no explanations):\"\n",
        "    )\n",
        "    inputs = tokenizer(template, return_tensors=\"pt\").to(device)\n",
        "    out = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=10,\n",
        "        num_beams=5,\n",
        "        early_stopping=True,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "    # slice off prompt tokens\n",
        "    gen_ids = out[0, inputs[\"input_ids\"].shape[-1]:]\n",
        "    gen_text = tokenizer.decode(gen_ids, skip_special_tokens=True).strip()\n",
        "    # return the very first whitespace‑delimited token\n",
        "    return gen_text.split()[0]"
      ],
      "metadata": {
        "id": "-F3q8YWHl0Wf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cot_answer(prompt: str) -> str:\n",
        "    template = (\n",
        "        \"Problem: \" + prompt.strip()\n",
        "        + \"\\n\\nLet's think step by step:\"\n",
        "    )\n",
        "    inputs = tokenizer(template, return_tensors=\"pt\").to(device)\n",
        "    out = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=200,\n",
        "        num_beams=1,\n",
        "        do_sample=True,\n",
        "        temperature=0.3,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        early_stopping=True\n",
        "    )\n",
        "    gen_ids = out[0, inputs[\"input_ids\"].shape[-1]:]\n",
        "    return tokenizer.decode(gen_ids, skip_special_tokens=True).strip()"
      ],
      "metadata": {
        "id": "gcHD2zJxl0Tm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Run Tests\n",
        "print(\"=== DIRECT ANSWERS ===\")\n",
        "for q in problems:\n",
        "    print(f\"Q: {q}\\nA: {direct_answer(q)}\\n\")\n",
        "\n",
        "print(\"=== CHAIN-OF-THOUGHT ===\")\n",
        "for q in problems:\n",
        "    print(f\"Q: {q}\\n{cot_answer(q)}\\n\" + \"-\"*40 + \"\\n\")"
      ],
      "metadata": {
        "id": "_7laXrLrl0RF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}