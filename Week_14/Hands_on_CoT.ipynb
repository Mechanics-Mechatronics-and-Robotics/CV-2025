{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOKVW1zMO1FIQ6yYx8rehqR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mechanics-Mechatronics-and-Robotics/CV-2025/blob/main/Week_14/Hands_on_CoT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 1: Install Dependencies\n",
        "!pip install -q torch transformers accelerate\n",
        "!pip install -q -U bitsandbytes  # Critical fix"
      ],
      "metadata": {
        "id": "wrEC18EoK359"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "import re"
      ],
      "metadata": {
        "id": "c6nLGZ3uRfJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Test Cases\n",
        "problems = [\n",
        "    \"A pizza is cut into 8 slices. If 3 people share equally, how many slices per person?\",\n",
        "    \"If a recipe calls for 3/4 cup of flour and you want to make half the recipe, how much flour do you need?\",\n",
        "    \"A car travels 240 km in 3 hours. What's its speed in km/h?\"\n",
        "]"
      ],
      "metadata": {
        "id": "EYmJ1iahQuxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 2: Load Quantized Model (Guaranteed to work)\n",
        "\n",
        "\n",
        "# Using Microsoft's Phi-2 (2.7B params but fits in T4 GPU with quantization)\n",
        "model_name = \"microsoft/phi-2\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16,\n",
        "    trust_remote_code=True\n",
        ")"
      ],
      "metadata": {
        "id": "b88BPwAuGetX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # @title Step 3: Optimized CoT Function\n",
        "# def generate_cot_response(prompt):\n",
        "#     cot_prompt = f\"\"\"Instruction: {prompt}\n",
        "#     Response: Let's think step by step:\"\"\"  # Force CoT\n",
        "\n",
        "#     inputs = tokenizer(cot_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "#     outputs = model.generate(**inputs, max_new_tokens=500)\n",
        "#     return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "# @title Phase 1: Direct Answering (No CoT)\n",
        "def direct_answer(prompt):\n",
        "    # Force ultra-short, no-explanation answers\n",
        "    inputs = tokenizer(\n",
        "        f\"{prompt} Answer ONLY with the final number or value, no text or explanation:\",\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=10,\n",
        "        num_beams=1,\n",
        "        temperature=0.0  # Completely deterministic\n",
        "    )\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# @title Phase 2: Explicit Chain-of-Thought\n",
        "def cot_answer(prompt):\n",
        "    # Demand structured step-by-step working\n",
        "    inputs = tokenizer(\n",
        "        f\"PROBLEM: {prompt}\\n\"\n",
        "        \"SOLUTION STEPS:\\n\"\n",
        "        \"1. \",\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=200,\n",
        "        temperature=0.3  # Slight creativity\n",
        "    )\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "gccSAD91Qdc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 4: Test with Math Problems\n",
        "problems = [\n",
        "    \"If a train travels 400 km in 4 hours, what's its speed?\",\n",
        "    \"You have 5 books, give 2 to a friend and buy 3 more. How many do you have?\"\n",
        "]\n",
        "\n",
        "for problem in problems:\n",
        "    print(f\"\\nProblem: {problem}\")\n",
        "    print(\"Solution:\", generate_cot_response(problem))"
      ],
      "metadata": {
        "id": "l_UFZsMFN3O9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Direct Answering\n",
        "def direct_answer(prompt):\n",
        "    # Force ultra-short answers with output constraints\n",
        "    inputs = tokenizer(\n",
        "        f\"{prompt} Answer ONLY with the final number, no explanation:\",\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=10,\n",
        "        num_beams=1,  # Disable creative exploration\n",
        "        temperature=0.0  # Pure deterministic output\n",
        "    )\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Test\n",
        "print(\"Pizza problem:\", direct_answer(\"A pizza is cut into 8 slices...\"))\n",
        "print(\"Flour problem:\", direct_answer(\"If a recipe calls for 3/4 cup...\"))"
      ],
      "metadata": {
        "id": "S7h-8uwROaBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cot_explicit(prompt):\n",
        "    # Demand structured working\n",
        "    inputs = tokenizer(\n",
        "        f\"PROBLEM: {prompt}\\nSOLUTION STEPS:\\n1.\",\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=200,\n",
        "        do_sample=True,  # Allow some creativity\n",
        "        temperature=0.3\n",
        "    )\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Test\n",
        "print(\"Pizza problem with CoT:\")\n",
        "print(cot_explicit(\"A pizza is cut into 8 slices...\"))"
      ],
      "metadata": {
        "id": "Ixn8NVYUP8Vx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Run Comparisons\n",
        "print(\"DIRECT ANSWERS (No CoT):\")\n",
        "for problem in problems:\n",
        "    print(f\"\\nQ: {problem}\")\n",
        "    print(f\"A: {direct_answer(problem)}\")\n",
        "\n",
        "print(\"\\n\\nCHAIN-OF-THOUGHT ANSWERS:\")\n",
        "for problem in problems:\n",
        "    print(f\"\\nQ: {problem}\")\n",
        "    print(cot_answer(problem))"
      ],
      "metadata": {
        "id": "mCX8CPJvQrW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# @title Load Model\n",
        "model_name = \"microsoft/phi-2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# @title Improved Direct Answer Function\n",
        "def direct_answer(prompt):\n",
        "    # More constrained prompt\n",
        "    full_prompt = f\"\"\"Question: {prompt}\n",
        "    Rules:\n",
        "    1. You must respond with ONLY the final numerical answer\n",
        "    2. No explanations or additional text\n",
        "    3. If the answer is a fraction, write it as x/y\n",
        "\n",
        "    Answer:\"\"\"\n",
        "\n",
        "    inputs = tokenizer(full_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=10,\n",
        "        num_beams=1,\n",
        "        temperature=0.0,\n",
        "        do_sample=False\n",
        "    )\n",
        "    # Extract just the number from output\n",
        "    full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return re.search(r'\\d+\\.?\\d*|\\d+/\\d+', full_output).group()\n",
        "\n",
        "# @title Improved CoT Function\n",
        "def cot_answer(prompt):\n",
        "    # Structured prompt template\n",
        "    full_prompt = f\"\"\"Solve this problem step-by-step:\n",
        "\n",
        "    Problem: {prompt}\n",
        "\n",
        "    Steps:\n",
        "    1.\"\"\"\n",
        "\n",
        "    inputs = tokenizer(full_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=200,\n",
        "        temperature=0.3,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# @title Test Cases\n",
        "problems = [\n",
        "    \"A pizza is cut into 8 slices. If 3 people share equally, how many slices per person?\",\n",
        "    \"If a recipe calls for 3/4 cup of flour and you want to make half the recipe, how much flour do you need?\",\n",
        "    \"A car travels 240 km in 3 hours. What's its speed in km/h?\"\n",
        "]\n",
        "\n",
        "# @title Run Tests\n",
        "print(\"=== DIRECT ANSWERS ===\")\n",
        "for p in problems:\n",
        "    print(f\"\\nQ: {p}\")\n",
        "    print(f\"A: {direct_answer(p)}\")\n",
        "\n",
        "print(\"\\n\\n=== CHAIN-OF-THOUGHT ===\")\n",
        "for p in problems:\n",
        "    print(f\"\\nQ: {p}\")\n",
        "    print(cot_answer(p))"
      ],
      "metadata": {
        "id": "fEqa6bCaRVgT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}