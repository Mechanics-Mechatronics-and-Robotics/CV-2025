{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mechanics-Mechatronics-and-Robotics/CV-2025/blob/main/Week_10/Contrastive_Learning_Tutorial_in_PyTorch_with_Point_Clouds.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTqAbOIjjSXm"
      },
      "source": [
        "## Contrastive Learning Tutorial in PyTorch with Point Clouds\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation / Setup"
      ],
      "metadata": {
        "id": "2-pAk_rLhhiV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iy3-tXq3i8dO",
        "outputId": "d8576d62-d289-4084-8af2-38908cbe7e73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â¬ Downloading https://github.com/jaimergp/miniforge/releases/download/24.11.2-1_colab/Miniforge3-colab-24.11.2-1_colab-Linux-x86_64.sh...\n",
            "ðŸ“¦ Installing...\n",
            "ðŸ“Œ Adjusting configuration...\n",
            "ðŸ©¹ Patching environment...\n",
            "â² Done in 0:00:26\n",
            "ðŸ” Restarting kernel...\n"
          ]
        }
      ],
      "source": [
        "# We will use conda for easier installation of PyG\n",
        "# If only using pip, it somehow takes forever to install on colab\n",
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "hI0CtCFQhZaL",
        "outputId": "0d2bdb74-c0a0-45ff-f592-2778e8e4777d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 853
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://data.pyg.org/whl/torch-2.6.0+cu124.html\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.6.0+cu124.html\n",
            "Collecting torch-scatter\n",
            "  Downloading torch_scatter-2.1.2.tar.gz (108 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31mÃ—\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31mâ”‚\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31mâ•°â”€>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31mÃ—\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31mâ•°â”€>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'torch_geometric'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-0f4d19295fcf>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch_geometric'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-0f4d19295fcf>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'echo $version'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric -f $version'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch_geometric'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# Install torch geometric for point-cloud layers\n",
        "import torch\n",
        "version = f\"https://data.pyg.org/whl/torch-{torch.__version__}.html\"\n",
        "try:\n",
        "    import torch_geometric\n",
        "except:\n",
        "    !echo $version\n",
        "    !pip install torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric -f $version\n",
        "    import torch_geometric"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "âœ‹ **Note: You'll need to restart your runtime and execute the two cells again.** âœ‹"
      ],
      "metadata": {
        "id": "sH4vLmajhONv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQDYyNK_-j--"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "Goal:\n",
        "- Self-Supervised Representation Learning of Shapes\n",
        "- Can be used for downstream tasks like clustering, fine-tuning, outlier-detection, ...\n",
        "- Pointcloud = Set of unconnected nodes --> PyG\n",
        "- [ShapeNet Dataset](https://paperswithcode.com/dataset/shapenet) - we just use a subset of classes and act like we didn't have labels\n",
        "- I select 5k data points as otherwise I run out of memory on Colab\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MtcN0Et2h21F"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.datasets import ShapeNet\n",
        "# Limit to 5000 samples, due to RAM restrictions\n",
        "dataset = ShapeNet(root=\".\", categories=[\"Table\", \"Lamp\", \"Guitar\", \"Motorbike\"]).shuffle()[:5000]\n",
        "print(\"Number of Samples: \", len(dataset))\n",
        "print(\"Sample: \", dataset[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attribute Name  | Description\n",
        "-------------------|------------------\n",
        "Pos       | Normalized positions as 3D coordinates\n",
        "X       |  Normal vectors\n",
        "Y       | Class label"
      ],
      "metadata": {
        "id": "X05pyLpSiHlB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tXjyMJP6B3bx"
      },
      "outputs": [],
      "source": [
        "dataset[0].pos"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's use plotly to inspect the data ..."
      ],
      "metadata": {
        "id": "SoaKy33ckunB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLuBtphVvlMZ"
      },
      "outputs": [],
      "source": [
        "#!pip install plotly --quiet\n",
        "import plotly.express as px\n",
        "\n",
        "def plot_3d_shape(shape):\n",
        "    print(\"Number of data points: \", shape.x.shape[0])\n",
        "    x = shape.pos[:, 0]\n",
        "    y = shape.pos[:, 1]\n",
        "    z = shape.pos[:, 2]\n",
        "    fig = px.scatter_3d(x=x, y=y, z=z, opacity=0.3)\n",
        "    fig.show()\n",
        "\n",
        "# Pick a sample\n",
        "sample_idx = 3\n",
        "plot_3d_shape(dataset[sample_idx])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's check the distribution of classes\n",
        "cat_dict = {key: 0 for key in dataset.categories}\n",
        "for datapoint in dataset: cat_dict[dataset.categories[datapoint.category.int()]]+=1\n",
        "cat_dict"
      ],
      "metadata": {
        "id": "_R4TFMbclLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEyNxSuPGrHX"
      },
      "source": [
        "## Data Preparation\n",
        "\n",
        "- In some scenarios it makes sense to pre-compute the augmentations (for example if heavy computations are involved)\n",
        "- This would require to store multiple Data Points in one Data Object, which is possible in PyTorch\n",
        "- Here we will compute the augmentations on the fly and use the below transformations for this\n",
        "- Later, for each data point we will need 2 augmentations (positive pair)\n",
        "- What are good augmentations for Point Clouds?\n",
        "    - Rotation (if the used layer is not rotation invariant)\n",
        "    - Jittering (can be seen as adding noise to the coordinates)\n",
        "    - Shifting / Shearing\n",
        "    - ... many more\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tlOnmFOUd9OP"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.loader import DataLoader\n",
        "import torch_geometric.transforms as T\n",
        "\n",
        "data_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# We're lucky and pytorch geometric helps us with pre-implemented transforms\n",
        "# which can also be applied on the whole batch directly\n",
        "augmentation = T.Compose([T.RandomJitter(0.03), T.RandomFlip(1), T.RandomShear(0.2)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ce7gMHZQH54Q"
      },
      "source": [
        "Let's have a look at some samples ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9rVbZMjDkjRm"
      },
      "outputs": [],
      "source": [
        "# Original data point\n",
        "sample = next(iter(data_loader))\n",
        "plot_3d_shape(sample[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JU_NCJ1Zjw4F"
      },
      "outputs": [],
      "source": [
        "# Augmented data point\n",
        "transformered = augmentation(sample)\n",
        "plot_3d_shape(transformered[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lle1XiGfEsn7"
      },
      "source": [
        "## Model\n",
        "\n",
        "- Different choices for Point Cloud Feature-Learning layers (PointNet, PointNet++, EdgeConv, PointTransformer, ...)\n",
        "- In PyTorch geometric we find an implementation of DynamicEdgeConv\n",
        "- It uses the parameter k to detect the nearest neighbors which form a subgraph\n",
        "- If you have many points, you can also sample a subset\n",
        "- In the paper they use 4 layers, here we just have 2\n",
        "- Implementation is inspired by [this PyG example](https://github.com/pyg-team/pytorch_geometric/blob/a6e349621d4caf8b381fe58f8e57109b2d0947ed/examples/dgcnn_segmentation.py)\n",
        "- We only apply augmentations during training\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i3G7KOpFIhjZ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.nn import Linear\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import MLP, DynamicEdgeConv, global_max_pool\n",
        "\n",
        "\n",
        "class Model(torch.nn.Module):\n",
        "    def __init__(self, k=20, aggr='max'):\n",
        "        super().__init__()\n",
        "        # Feature extraction\n",
        "        self.conv1 = DynamicEdgeConv(MLP([2 * 3, 64, 64]), k, aggr)\n",
        "        self.conv2 = DynamicEdgeConv(MLP([2 * 64, 128]), k, aggr)\n",
        "        # Encoder head\n",
        "        self.lin1 = Linear(128 + 64, 128)\n",
        "        # Projection head (See explanation in SimCLRv2)\n",
        "        self.mlp = MLP([128, 256, 32], norm=None)\n",
        "\n",
        "    def forward(self, data, train=True):\n",
        "        if train:\n",
        "            # Get 2 augmentations of the batch\n",
        "            augm_1 = augmentation(data)\n",
        "            augm_2 = augmentation(data)\n",
        "\n",
        "            # Extract properties\n",
        "            pos_1, batch_1 = augm_1.pos, augm_1.batch\n",
        "            pos_2, batch_2 = augm_2.pos, augm_2.batch\n",
        "\n",
        "            # Get representations for first augmented view\n",
        "            x1 = self.conv1(pos_1, batch_1)\n",
        "            x2 = self.conv2(x1, batch_1)\n",
        "            h_points_1 = self.lin1(torch.cat([x1, x2], dim=1))\n",
        "\n",
        "            # Get representations for second augmented view\n",
        "            x1 = self.conv1(pos_2, batch_2)\n",
        "            x2 = self.conv2(x1, batch_2)\n",
        "            h_points_2 = self.lin1(torch.cat([x1, x2], dim=1))\n",
        "\n",
        "            # Global representation\n",
        "            h_1 = global_max_pool(h_points_1, batch_1)\n",
        "            h_2 = global_max_pool(h_points_2, batch_2)\n",
        "        else:\n",
        "            x1 = self.conv1(data.pos, data.batch)\n",
        "            x2 = self.conv2(x1, data.batch)\n",
        "            h_points = self.lin1(torch.cat([x1, x2], dim=1))\n",
        "            return global_max_pool(h_points, data.batch)\n",
        "\n",
        "        # Transformation for loss function\n",
        "        compact_h_1 = self.mlp(h_1)\n",
        "        compact_h_2 = self.mlp(h_2)\n",
        "        return h_1, h_2, compact_h_1, compact_h_2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAh2QgX9Nof1"
      },
      "source": [
        "Possible improvement: Only pass once through model by stacking augmentations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0QjhMfhJ8W7"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- We use InfoNCE / NT-Xent Loss implemented in pytorch metric learning library\n",
        "- Temperature allows to balance the similarity measure (make it more peaked)\n",
        "- Typical values are around 0.1 / 0.2"
      ],
      "metadata": {
        "id": "fI18VkeQxadF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FGxTDy0sLDTp"
      },
      "outputs": [],
      "source": [
        "# See https://kevinmusgrave.github.io/pytorch-metric-learning/losses/#ntxentloss\n",
        "!pip install pytorch-metric-learning -q\n",
        "\n",
        "from pytorch_metric_learning.losses import NTXentLoss\n",
        "loss_func = NTXentLoss(temperature=0.10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDplPASCKNRD"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = Model().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "\n",
        "# Use a large batch size (might lead to RAM issues)\n",
        "# Free Colab Version has ~ 12 GB of RAM\n",
        "data_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- No test dataset, as the evaluation can be done \"downstream\"\n",
        "- The compact representations go into the loss function\n",
        "- During test time no augmentations are applied and we can use the output representations"
      ],
      "metadata": {
        "id": "LASK3n1qz8Fx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXLT7Up7Kd9g"
      },
      "outputs": [],
      "source": [
        "import tqdm\n",
        "\n",
        "def train():\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for _, data in enumerate(tqdm.tqdm(data_loader)):\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        # Get data representations\n",
        "        h_1, h_2, compact_h_1, compact_h_2 = model(data)\n",
        "        # Prepare for loss\n",
        "        embeddings = torch.cat((compact_h_1, compact_h_2))\n",
        "        # The same index corresponds to a positive pair\n",
        "        indices = torch.arange(0, compact_h_1.size(0), device=compact_h_2.device)\n",
        "        labels = torch.cat((indices, indices))\n",
        "        loss = loss_func(embeddings, labels)\n",
        "        loss.backward()\n",
        "        total_loss += loss.item() * data.num_graphs\n",
        "        optimizer.step()\n",
        "    return total_loss / len(dataset)\n",
        "\n",
        "for epoch in range(1, 4):\n",
        "    loss = train()\n",
        "    print(f'Epoch {epoch:03d}, Loss: {loss:.4f}')\n",
        "    scheduler.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J49zjaa5cxRe"
      },
      "source": [
        "## Evaluation of the Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z97R1Vhou0uQ"
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get sample batch\n",
        "sample = next(iter(data_loader))\n",
        "\n",
        "# Get representations\n",
        "h = model(sample.to(device), train=False)\n",
        "h = h.cpu().detach()\n",
        "labels = sample.category.cpu().detach().numpy()\n",
        "\n",
        "# Get low-dimensional t-SNE Embeddings\n",
        "h_embedded = TSNE(n_components=2, learning_rate='auto',\n",
        "                   init='random').fit_transform(h.numpy())\n",
        "\n",
        "# Plot\n",
        "ax = sns.scatterplot(x=h_embedded[:,0], y=h_embedded[:,1], hue=labels,\n",
        "                    alpha=0.5, palette=\"tab10\")\n",
        "\n",
        "# Add labels to be able to identify the data points\n",
        "annotations = list(range(len(h_embedded[:,0])))\n",
        "\n",
        "def label_points(x, y, val, ax):\n",
        "    a = pd.concat({'x': x, 'y': y, 'val': val}, axis=1)\n",
        "    for i, point in a.iterrows():\n",
        "        ax.text(point['x']+.02, point['y'], str(int(point['val'])))\n",
        "\n",
        "label_points(pd.Series(h_embedded[:,0]),\n",
        "            pd.Series(h_embedded[:,1]),\n",
        "            pd.Series(annotations),\n",
        "            plt.gca())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdEaaGAGY7X_"
      },
      "source": [
        "Let's find the most similar and most different data points ...\n",
        "\n",
        "[Source](https://stackoverflow.com/questions/50411191/how-to-compute-the-cosine-similarity-in-pytorch-for-all-rows-in-a-matrix-with-re)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NwEq0v63Y4F4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def sim_matrix(a, b, eps=1e-8):\n",
        "    \"\"\"\n",
        "    Eps for numerical stability\n",
        "    \"\"\"\n",
        "    a_n, b_n = a.norm(dim=1)[:, None], b.norm(dim=1)[:, None]\n",
        "    a_norm = a / torch.max(a_n, eps * torch.ones_like(a_n))\n",
        "    b_norm = b / torch.max(b_n, eps * torch.ones_like(b_n))\n",
        "    sim_mt = torch.mm(a_norm, b_norm.transpose(0, 1))\n",
        "    return sim_mt\n",
        "\n",
        "similarity = sim_matrix(h, h)\n",
        "max_indices = torch.topk(similarity, k=2)[1][:, 1]\n",
        "max_vals  = torch.topk(similarity, k=2)[0][:, 1]\n",
        "\n",
        "# Select index\n",
        "idx = 17\n",
        "similar_idx = max_indices[idx]\n",
        "print(f\"Most similar data point in the embedding space for {idx} is {similar_idx}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZJvNho8nQoW"
      },
      "source": [
        "Categories are: \"Table\", \"Lamp\", \"Guitar\", \"Motorbike\", \"Skateboard\"\n",
        "\n",
        "**Note**: This is only based on the data in the current batch!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qMv3cc_bb6ag"
      },
      "outputs": [],
      "source": [
        "plot_3d_shape(sample[idx].cpu())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i7Q86ALucDEx"
      },
      "outputs": [],
      "source": [
        "plot_3d_shape(sample[similar_idx].cpu())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This confirms that our embedding space has a proper arrangement and that our contrastive loss separated different entities successfully."
      ],
      "metadata": {
        "id": "kppFmNVKa2K6"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "2-pAk_rLhhiV",
        "DQDYyNK_-j--",
        "rEyNxSuPGrHX",
        "Lle1XiGfEsn7",
        "U0QjhMfhJ8W7",
        "J49zjaa5cxRe"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}