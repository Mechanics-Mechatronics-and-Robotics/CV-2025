{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/damirnurtdinov/miniconda3/envs/py39/lib/python3.9/site-packages/torchvision/io/image.py:14: UserWarning: Failed to load image Python extension: 'dlopen(/Users/damirnurtdinov/miniconda3/envs/py39/lib/python3.9/site-packages/torchvision/image.so, 0x0006): Library not loaded: @rpath/libjpeg.9.dylib\n",
      "  Referenced from: <FB2FD416-6C4D-3621-B677-61F07C02A3C5> /Users/damirnurtdinov/miniconda3/envs/py39/lib/python3.9/site-packages/torchvision/image.so\n",
      "  Reason: tried: '/Users/damirnurtdinov/miniconda3/envs/py39/lib/python3.9/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/Users/damirnurtdinov/miniconda3/envs/py39/lib/python3.9/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/Users/damirnurtdinov/miniconda3/envs/py39/lib/python3.9/lib-dynload/../../libjpeg.9.dylib' (no such file), '/Users/damirnurtdinov/miniconda3/envs/py39/bin/../lib/libjpeg.9.dylib' (no such file)'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "/Users/damirnurtdinov/miniconda3/envs/py39/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#Pytorch modules\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, random_split, TensorDataset\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision import datasets, transforms, models\n",
    "#scipy\n",
    "from scipy.stats import mode\n",
    "#sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "#Numpy\n",
    "import numpy as np\n",
    "#Pandas\n",
    "import pandas as pd\n",
    "#Lightning & logging\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "#Data observation\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import requests\n",
    "from pathlib import Path\n",
    "#Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#Logging\n",
    "from clearml import Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "# Function for setting the seed to implement parallel tests\n",
    "SEEDS = [42, 0, 17, 9, 3, 16, 2]\n",
    "SEED = 42 # random seed by default\n",
    "pl.seed_everything(SEED)\n",
    "\n",
    "# Determine the device (GPU if available, otherwise CPU)\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device('cpu') # I have M1 chip\n",
    "\n",
    "# Prioritizes speed but may reduce precision\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "# # Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.use_deterministic_algorithms(True)\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'CIFAR10N' # dataset with the real-world noise\n",
    "# Can be 'clean_label', 'worse_label', 'aggre_label', 'random_label1', 'random_label2', 'random_label3'\n",
    "NOISE_TYPE = 'worse_label'\n",
    "\n",
    "NS = {\n",
    "    'train': 45000,\n",
    "    'val': 5000,\n",
    "    'test': 10000\n",
    "} # for MNIST\n",
    "\n",
    "SIZE = 32 #image size\n",
    "NUM_CLASSES = 10\n",
    "CLASS_NAMES = ['plane', 'car', 'bird', 'cat',\n",
    "               'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "#For the MNIST dataset\n",
    "MEAN = np.array([0.491,0.482,0.447])\n",
    "STD  = np.array([0.247,0.243,0.261])\n",
    "\n",
    "#Model parameters\n",
    "LOSS_FUN = 'N' # 'CE','CELoss'(custom), 'N', 'B', etc.\n",
    "ARCHITECTURE = 'CNN' # 'CNN, 'ResNet50', 'ViT', etc.\n",
    "\n",
    "#Collect the parameters (hyperparams and others)\n",
    "hparams = {\n",
    "    \"seed\": SEED,\n",
    "    \"lr\": 0.001,\n",
    "    'weight_decay': 0.0,\n",
    "    \"dropout\": 0.0,\n",
    "    \"bs\": 128,\n",
    "    \"num_workers\": 0, #set 2 in Colab, or 0 in InnoDataHub\n",
    "    \"num_epochs\": 20,\n",
    "    \"criterion\": LOSS_FUN,\n",
    "    \"architecture\": ARCHITECTURE,\n",
    "    \"num_samples\": NS,\n",
    "    \"im_size\": SIZE,\n",
    "    \"mean\": np.array([0.4914, 0.4822, 0.4465]),\n",
    "    \"std\": np.array([0.2470, 0.2435, 0.2616]),\n",
    "    'randResCrop': {'size': (SIZE, SIZE), 'scale': (0.8, 1.0), 'ratio': (0.9, 1.1)},\n",
    "    \"n_classes\": NUM_CLASSES,\n",
    "    \"noise_path\": './data/CIFAR-10_human.pt',\n",
    "    \"noise_type\": NOISE_TYPE  # Can be 'clean_label', 'worse_label', 'aggre_label', etc.\n",
    "}\n",
    "\n",
    "#Visualization\n",
    "vis_params = {\n",
    "    'fig_size': 5,\n",
    "    'num_samples': 5,\n",
    "    'num_bins': 50,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(url, save_path):\n",
    "    \"\"\"Download a file from a URL and save it to the specified path.\"\"\"\n",
    "    response = requests.get(url, stream=True)\n",
    "    if response.status_code == 200:\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)  # Ensure directory exists\n",
    "        with open(save_path, 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "        print(f\"File downloaded and saved to {save_path}\")\n",
    "    else:\n",
    "        raise Exception(f\"Failed to download file from {url}. Status code: {response.status_code}\")\n",
    "    \n",
    "class CIFAR10(datasets.CIFAR10):\n",
    "    \"\"\"CIFAR10 dataset with noisy labels.\"\"\"\n",
    "    def __init__(self, root, train=True, transform=None, target_transform=None,\n",
    "                 download=False, noise_type=None, noise_path=None, is_human=True):\n",
    "        super().__init__(root, train=train, transform=transform,\n",
    "                         target_transform=target_transform, download=download)\n",
    "        self.noise_type = noise_type\n",
    "        self.noise_path = noise_path\n",
    "        self.is_human = is_human\n",
    "\n",
    "        if self.train and self.noise_type is not None:\n",
    "            self.load_noisy_labels()\n",
    "\n",
    "    def load_noisy_labels(self):\n",
    "        noise_file = torch.load(self.noise_path)\n",
    "        if isinstance(noise_file, dict):\n",
    "            if \"clean_label\" in noise_file.keys():\n",
    "                clean_label = torch.tensor(noise_file['clean_label'])\n",
    "                assert torch.sum(torch.tensor(self.targets) - clean_label) == 0\n",
    "                print(f'Loaded {self.noise_type} from {self.noise_path}.')\n",
    "                print(f'The overall noise rate is {1 - np.mean(clean_label.numpy() == noise_file[self.noise_type])}')\n",
    "            self.noisy_labels = noise_file[self.noise_type].reshape(-1)\n",
    "        else:\n",
    "            raise Exception('Input Error')\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, target = super().__getitem__(index)\n",
    "        if self.train and self.noise_type is not None:\n",
    "            target = self.noisy_labels[index]\n",
    "        return img, target, index\n",
    "    \n",
    "class CIFAR10DataModule(pl.LightningDataModule):\n",
    "    def __init__(self, params):\n",
    "        super().__init__()\n",
    "        self.seed = params['seed']\n",
    "        self.batch_size = params['bs']\n",
    "        self.num_workers = params['num_workers']\n",
    "        self.mean = params['mean']\n",
    "        self.std = params['std']\n",
    "        self.ns = params['num_samples']\n",
    "        self.rand_res_crop = params['randResCrop']\n",
    "        self.noise_path = params.get('noise_path', './data/CIFAR-10_human.pt')\n",
    "        self.noise_type = params.get('noise_type', 'worse_label')  # Default to 'worse_label'\n",
    "\n",
    "        # Ensure the data directory exists\n",
    "        os.makedirs(os.path.dirname(self.noise_path), exist_ok=True)\n",
    "\n",
    "        # Download the CIFAR-10_human.pt file if it doesn't exist\n",
    "        if not os.path.exists(self.noise_path):\n",
    "            print(f\"Downloading CIFAR-10_human.pt from GitHub...\")\n",
    "            download_file(\n",
    "                url=\"https://github.com/UCSC-REAL/cifar-10-100n/raw/main/data/CIFAR-10_human.pt\",\n",
    "                save_path=self.noise_path\n",
    "            )\n",
    "\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(size=self.rand_res_crop['size'],\n",
    "                                         scale=self.rand_res_crop['scale'],\n",
    "                                         ratio=self.rand_res_crop['ratio']),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(self.mean, self.std)\n",
    "        ])\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # Download CIFAR-10 dataset\n",
    "        datasets.CIFAR10(root='./data', train=True, download=True)\n",
    "        datasets.CIFAR10(root='./data', train=False, download=True)\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # Load noisy labels\n",
    "        noise_file = torch.load(self.noise_path)\n",
    "        clean_label = noise_file['clean_label']\n",
    "        noisy_label = noise_file[self.noise_type]\n",
    "\n",
    "        # Split dataset into train and validation sets\n",
    "        cifar10_full = CIFAR10(root='./data', train=True, transform=self.transform,\n",
    "                               noise_type=self.noise_type, noise_path=self.noise_path, is_human=True)\n",
    "        pl.seed_everything(self.seed)\n",
    "        self.cifar10_train, self.cifar10_val = random_split(cifar10_full,\n",
    "                                                            [self.ns['train'],\n",
    "                                                             self.ns['val']])\n",
    "        self.cifar10_test = CIFAR10(root='./data', train=False, transform=self.transform)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.cifar10_train, batch_size=self.batch_size,\n",
    "                          num_workers=self.num_workers, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.cifar10_val, batch_size=self.batch_size,\n",
    "                          num_workers=self.num_workers)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.cifar10_test, batch_size=self.batch_size,\n",
    "                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_bn(bn, x):\n",
    "    return bn(x)\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_channel=3, n_outputs=10, dropout_rate=0.25, top_bn=False):\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.top_bn = top_bn\n",
    "        super(CNN, self).__init__()\n",
    "        self.c1=nn.Conv2d(input_channel,128,kernel_size=3,stride=1, padding=1)\n",
    "        self.c2=nn.Conv2d(128,128,kernel_size=3,stride=1, padding=1)\n",
    "        self.c3=nn.Conv2d(128,128,kernel_size=3,stride=1, padding=1)\n",
    "        self.c4=nn.Conv2d(128,256,kernel_size=3,stride=1, padding=1)\n",
    "        self.c5=nn.Conv2d(256,256,kernel_size=3,stride=1, padding=1)\n",
    "        self.c6=nn.Conv2d(256,256,kernel_size=3,stride=1, padding=1)\n",
    "        self.c7=nn.Conv2d(256,512,kernel_size=3,stride=1, padding=0)\n",
    "        self.c8=nn.Conv2d(512,256,kernel_size=3,stride=1, padding=0)\n",
    "        self.c9=nn.Conv2d(256,128,kernel_size=3,stride=1, padding=0)\n",
    "        self.l_c1=nn.Linear(128,n_outputs)\n",
    "        self.bn1=nn.BatchNorm2d(128)\n",
    "        self.bn2=nn.BatchNorm2d(128)\n",
    "        self.bn3=nn.BatchNorm2d(128)\n",
    "        self.bn4=nn.BatchNorm2d(256)\n",
    "        self.bn5=nn.BatchNorm2d(256)\n",
    "        self.bn6=nn.BatchNorm2d(256)\n",
    "        self.bn7=nn.BatchNorm2d(512)\n",
    "        self.bn8=nn.BatchNorm2d(256)\n",
    "        self.bn9=nn.BatchNorm2d(128)\n",
    "\n",
    "    def forward(self, x,):\n",
    "        h=x\n",
    "        h=self.c1(h)\n",
    "        h=F.leaky_relu(call_bn(self.bn1, h), negative_slope=0.01)\n",
    "        h=self.c2(h)\n",
    "        h=F.leaky_relu(call_bn(self.bn2, h), negative_slope=0.01)\n",
    "        h=self.c3(h)\n",
    "        h=F.leaky_relu(call_bn(self.bn3, h), negative_slope=0.01)\n",
    "        h=F.max_pool2d(h, kernel_size=2, stride=2)\n",
    "        h=F.dropout2d(h, p=self.dropout_rate)\n",
    "\n",
    "        h=self.c4(h)\n",
    "        h=F.leaky_relu(call_bn(self.bn4, h), negative_slope=0.01)\n",
    "        h=self.c5(h)\n",
    "        h=F.leaky_relu(call_bn(self.bn5, h), negative_slope=0.01)\n",
    "        h=self.c6(h)\n",
    "        h=F.leaky_relu(call_bn(self.bn6, h), negative_slope=0.01)\n",
    "        h=F.max_pool2d(h, kernel_size=2, stride=2)\n",
    "        h=F.dropout2d(h, p=self.dropout_rate)\n",
    "\n",
    "        h=self.c7(h)\n",
    "        h=F.leaky_relu(call_bn(self.bn7, h), negative_slope=0.01)\n",
    "        h=self.c8(h)\n",
    "        h=F.leaky_relu(call_bn(self.bn8, h), negative_slope=0.01)\n",
    "        h=self.c9(h)\n",
    "        h=F.leaky_relu(call_bn(self.bn9, h), negative_slope=0.01)\n",
    "        h=F.avg_pool2d(h, kernel_size=h.data.shape[2])\n",
    "\n",
    "        h = h.view(h.size(0), h.size(1))\n",
    "        logit=self.l_c1(h)\n",
    "        if self.top_bn:\n",
    "            logit=call_bn(self.bn_c1, logit)\n",
    "        return logit\n",
    "    \n",
    "class ResNet50(nn.Module):\n",
    "    def __init__(self, n_outputs, freeze=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_outputs (int): Number of output classes.\n",
    "            freeze (bool): If True, freeze all layers except the head.\n",
    "        \"\"\"\n",
    "        super(ResNet50, self).__init__()\n",
    "        self.n_outputs = n_outputs\n",
    "        self.freeze = freeze\n",
    "\n",
    "        # Load the pre-trained ResNet50 model\n",
    "        self.resnet50 = models.resnet50(pretrained=True)\n",
    "\n",
    "        # Modify the final layer to match the number of outputs\n",
    "        self.resnet50.fc = nn.Linear(self.resnet50.fc.in_features, n_outputs)\n",
    "\n",
    "        # Freeze all layers except the head if freeze=True\n",
    "        if self.freeze:\n",
    "            self._freeze_layers()\n",
    "\n",
    "    def _freeze_layers(self):\n",
    "        \"\"\"\n",
    "        Freeze all layers except the head.\n",
    "        \"\"\"\n",
    "        # Freeze all parameters in the model\n",
    "        for param in self.resnet50.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Unfreeze the final classification layer (head)\n",
    "        for param in self.resnet50.fc.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet50(x)\n",
    "\n",
    "def count_trainable_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, n_outputs, freeze=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_outputs (int): Number of output classes.\n",
    "            freeze (bool): If True, freeze all layers except the head.\n",
    "        \"\"\"\n",
    "        super(ViT, self).__init__()\n",
    "        self.n_outputs = n_outputs\n",
    "        self.freeze = freeze\n",
    "\n",
    "        # Load the pre-trained ViT model\n",
    "        self.vit = models.vit_b_16(pretrained=True)\n",
    "\n",
    "        # Modify the final layer to match the number of outputs\n",
    "        self.vit.heads.head = nn.Linear(self.vit.heads.head.in_features, n_outputs)\n",
    "\n",
    "        # Freeze all layers except the head if freeze=True\n",
    "        if self.freeze:\n",
    "            self._freeze_layers()\n",
    "\n",
    "    def _freeze_layers(self):\n",
    "        \"\"\"\n",
    "        Freeze all layers except the head.\n",
    "        \"\"\"\n",
    "        # Freeze all parameters in the model\n",
    "        for param in self.vit.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Unfreeze the head (final classification layer)\n",
    "        for param in self.vit.heads.head.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.vit(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(dataloader,model,hparams=hparams,loss_fn_red=None):\n",
    "    # Collect images, predictions, and losses\n",
    "    # images = []\n",
    "    preds  = []\n",
    "    labels = []\n",
    "    losses = []\n",
    "    correct= 0\n",
    "    total  = 0\n",
    "    for batch in dataloader:\n",
    "        x, y, _ = batch\n",
    "        with torch.no_grad():\n",
    "            logits = model(x)\n",
    "            # loss = loss_fn_red(h,y)\n",
    "            pred = torch.argmax(logits[:,:hparams['n_classes']], dim=1)\n",
    "        correct += (pred == y).sum().item()  # Number of correct predictions\n",
    "        total += y.size(0)  # Total number of samples\n",
    "\n",
    "        # images.extend(x.cpu())\n",
    "        preds.extend(pred.cpu().numpy())\n",
    "        labels.extend(y.cpu().numpy())\n",
    "        # losses.extend(loss.cpu().numpy())\n",
    "    acc = correct / total\n",
    "    return preds, labels, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6j/s19nhfjj4pv14vny7nrq8lf00000gn/T/ipykernel_6271/2517412809.py:83: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  noise_file = torch.load(self.noise_path)\n",
      "/var/folders/6j/s19nhfjj4pv14vny7nrq8lf00000gn/T/ipykernel_6271/2517412809.py:27: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  noise_file = torch.load(self.noise_path)\n",
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded worse_label from ./data/CIFAR-10_human.pt.\n",
      "The overall noise rate is 0.40208\n"
     ]
    }
   ],
   "source": [
    "data_module = CIFAR10DataModule(hparams)\n",
    "data_module.prepare_data()\n",
    "data_module.setup()\n",
    "test_dataloader = data_module.test_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class train_model(pl.LightningModule):\n",
    "    def __init__(self, model=None, loss=None, hparams=hparams):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(hparams)\n",
    "        self.model = model\n",
    "        self.loss_fn = loss\n",
    "        self.nc = hparams['n_classes']\n",
    "        self.lr = hparams['lr']\n",
    "        self.wd = hparams['weight_decay']\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y, _ = batch  # Unpack batch (ignore indices for now)\n",
    "        logits = self(x)\n",
    "        loss = self.loss_fn(logits, y)\n",
    "\n",
    "        # Log training loss and accuracy\n",
    "        # preds = torch.argmax(logits[:, :self.nc], dim=1)\n",
    "        # acc = (preds == y).float().mean()\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        # self.log('train_acc', acc, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y, _ = batch  # Unpack batch (ignore indices for now)\n",
    "        logits = self(x)\n",
    "        loss = self.loss_fn(logits, y)\n",
    "\n",
    "        # Log validation loss and accuracy\n",
    "        # preds = torch.argmax(logits[:, :self.nc], dim=1)\n",
    "        # acc = (preds == y).float().mean()\n",
    "        self.log('val_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        # self.log('val_acc', acc, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y, _ = batch  # Unpack batch (ignore indices for now)\n",
    "        logits = self(x)\n",
    "        loss = self.loss_fn(logits, y)\n",
    "\n",
    "        # Log test loss and accuracy\n",
    "        preds = torch.argmax(logits[:, :self.nc], dim=1)\n",
    "        acc = (preds == y).float().mean()\n",
    "        self.log('test_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log('test_acc', acc, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return {'loss': loss, 'preds': preds, 'y': y}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr, weight_decay=self.wd)\n",
    "\n",
    "        # Optionally, add a learning rate scheduler\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=1.0)\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BLoss(nn.Module):\n",
    "    def __init__(self, params=hparams):\n",
    "        super(BLoss, self).__init__()\n",
    "        self.smoothing = params.get('label_smoothing', 0.0)\n",
    "        self.num_classes = params.get('n_classes', 10)\n",
    "        self.inv_smoothing = 1.0 - self.smoothing  # Probability for the correct class\n",
    "        self.eps = 1e-10  # Small epsilon for numerical stability\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        \"\"\"\n",
    "        x: Model output (logits + certainty)\n",
    "            - x[:, :self.num_classes]: Logits for class probabilities\n",
    "            - x[:, self.num_classes:]: Certainty values\n",
    "        y: Ground truth labels (class indices)\n",
    "        \"\"\"\n",
    "        # Ensure y is a tensor of class indices\n",
    "        if y.dtype != torch.long:\n",
    "            y = y.long()\n",
    "\n",
    "        # Extract certainty and probabilities from the model output\n",
    "        certainty = torch.sigmoid(x[:, self.num_classes:])  # Certainty values (batch_size, 1)\n",
    "        logits = x[:, :self.num_classes]  # Logits for class probabilities (batch_size, num_classes)\n",
    "        prob = F.softmax(logits, dim=1)  # Softmax probabilities (batch_size, num_classes)\n",
    "\n",
    "        # Apply label smoothing to the one-hot encoded labels\n",
    "        with torch.no_grad():\n",
    "            yoh = torch.zeros_like(logits)\n",
    "            yoh.fill_(self.smoothing / (self.num_classes - 1))\n",
    "            yoh.scatter_(1, y.unsqueeze(1), self.inv_smoothing)\n",
    "\n",
    "        # Compute cosine similarity between predictions and labels\n",
    "        cos = nn.CosineSimilarity(dim=1)\n",
    "        cosyh = cos(yoh, prob)  # Cosine similarity (batch_size,)\n",
    "\n",
    "        # Compute the terms of the loss\n",
    "        delta = yoh * prob  # Element-wise product of one-hot labels and probabilities\n",
    "        entropy_term = delta * torch.log(delta + self.eps)  # Entropy term (avoid log(0))\n",
    "\n",
    "        # Loss terms\n",
    "        loss0 = -cosyh * torch.log(certainty.squeeze() / self.num_classes + self.eps)  # First term\n",
    "        loss1 = -(self.num_classes - 1) * (1 - cosyh) * torch.log((1 - certainty.squeeze()) / self.num_classes + self.eps)  # Second term\n",
    "\n",
    "        # Combine the terms and compute the mean over the batch\n",
    "        loss = (entropy_term.sum(dim=1) + loss0 + loss1).mean()\n",
    "\n",
    "        return loss\n",
    "    \n",
    "class NLoss(nn.Module):\n",
    "    def __init__(self, params=hparams):\n",
    "        super(NLoss, self).__init__()\n",
    "        self.smoothing =   params.get('label_smoothing', 0.0)\n",
    "        self.num_classes = params.get('n_classes', 10)\n",
    "        self.inv_smoothing = 1.0 - self.smoothing  # Probability for the correct class\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        \"\"\"\n",
    "        x: Model output (logits + log variance)\n",
    "            - x[:, :self.num_classes]: Logits for class probabilities (h)\n",
    "            - x[:, self.num_classes:]: Logarithmic variance (s)\n",
    "        y: Labels\n",
    "        \"\"\"\n",
    "        # Split the model output into predictions (h) and log variance (s)\n",
    "        logits = x[:, :self.num_classes]  # Predictions (h)\n",
    "        log_var = x[:, self.num_classes:]  # Logarithmic variance (s)\n",
    "\n",
    "        # Apply label smoothing to the one-hot encoded labels\n",
    "        with torch.no_grad():\n",
    "            yoh = torch.zeros_like(logits)\n",
    "            yoh.fill_(self.smoothing / (self.num_classes - 1))\n",
    "            yoh.scatter_(1, y.data.unsqueeze(1), self.inv_smoothing)\n",
    "\n",
    "        # Compute the squared differences between predictions and smoothed labels\n",
    "        # logits = torch.softmax(logits, dim=1)  # Convert logits to probabilities\n",
    "        squared_diff = torch.pow(yoh - logits, 2)  # (y_k - h_k)^2\n",
    "\n",
    "        # Compute the exponential of the negative log variance (e^{-s})\n",
    "        # log_var = torch.clamp(log_var, min=-10, max=10)  # Clamp log_var to a reasonable range\n",
    "        exp_neg_log_var = torch.exp(-log_var)\n",
    "\n",
    "        # Compute the first term of the loss: e^{-s} * sum((y_k - h_k)^2)\n",
    "        term1 = exp_neg_log_var * squared_diff.sum(dim=1)\n",
    "\n",
    "        # Compute the second term of the loss: N * s\n",
    "        term2 = self.num_classes * log_var\n",
    "\n",
    "        # Combine the terms and compute the mean over the batch\n",
    "        loss = (term1 + term2).mean()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_arch_and_loss(hparams):\n",
    "    \"\"\"\n",
    "    Returns the architecture and loss function based on the provided hparams.\n",
    "\n",
    "    Args:\n",
    "        hparams (dict): Hyperparameters dictionary, including 'ARCHITECTURE' and 'criterion'.\n",
    "\n",
    "    Returns:\n",
    "        arch: The model architecture.\n",
    "        loss: The loss function.\n",
    "    \"\"\"\n",
    "    # Determine the number of outputs based on the loss function\n",
    "    if hparams['criterion'] in ['B', 'N']:\n",
    "        n_outputs = hparams['n_classes'] + 1  # Add 1 output neuron for BLoss or NLoss\n",
    "    else:\n",
    "        n_outputs = hparams['n_classes']  # Default number of outputs\n",
    "    # print(n_outputs)\n",
    "    # Define the architectures\n",
    "    architectures = {\n",
    "        'CNN': CNN(n_outputs=n_outputs),\n",
    "        'ResNet50': ResNet50(n_outputs=n_outputs, freeze=hparams.get('freeze', False)),\n",
    "        'ViT': ViT(n_outputs=n_outputs, freeze=hparams.get('freeze', False)),\n",
    "    }\n",
    "\n",
    "    # Define the loss functions\n",
    "    losses = {\n",
    "        'CE': nn.CrossEntropyLoss(),\n",
    "        'B': BLoss(),\n",
    "        'N': NLoss(),\n",
    "    }\n",
    "\n",
    "    # Get the architecture and loss based on hparams\n",
    "    arch = architectures.get(hparams['architecture'])\n",
    "    loss = losses.get(hparams['criterion'])\n",
    "\n",
    "    if arch is None:\n",
    "        raise ValueError(f\"Architecture '{hparams['ARCHITECTURE']}' is not supported.\")\n",
    "    if loss is None:\n",
    "        raise ValueError(f\"Loss function '{hparams['criterion']}' is not supported.\")\n",
    "\n",
    "    return arch, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/damirnurtdinov/miniconda3/envs/py39/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/damirnurtdinov/miniconda3/envs/py39/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/Users/damirnurtdinov/miniconda3/envs/py39/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ViT_B_16_Weights.IMAGENET1K_V1`. You can also use `weights=ViT_B_16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "architectures = {\n",
    "        'CNN': CNN(n_outputs=10),\n",
    "        'ResNet50': ResNet50(n_outputs=10, freeze=hparams.get('freeze', False)),\n",
    "        'ViT': ViT(n_outputs=10, freeze=hparams.get('freeze', False)),\n",
    "    }\n",
    "\n",
    "losses = {\n",
    "    'CE': nn.CrossEntropyLoss(),\n",
    "    'B': BLoss(),\n",
    "    'N': NLoss(),\n",
    "}\n",
    "\n",
    "# Function to load a model from a checkpoint\n",
    "def load_model(architecture, loss, seed, hparams):\n",
    "    # Use glob to find all matching checkpoint files\n",
    "    pattern = f\"checkpoints/model-{architecture}-{loss}-seed-{seed}-epoch=*.ckpt\"\n",
    "    matching_files = glob.glob(pattern)\n",
    "    \n",
    "    if not matching_files:\n",
    "        raise FileNotFoundError(f\"No checkpoint found for {architecture}, {loss}, seed {seed}\")\n",
    "    \n",
    "    # Load the first matching file (or choose the latest epoch if needed)\n",
    "    model_path = matching_files[0]\n",
    "    \n",
    "    arch, loss_fn = get_arch_and_loss(hparams)\n",
    "    \n",
    "    # Instantiate the model and load the checkpoint\n",
    "    # model = architectures[architecture]\n",
    "    # loss_fn = losses[loss]\n",
    "    best_model = train_model.load_from_checkpoint(model_path, model=arch, loss=loss_fn)\n",
    "    best_model.eval()\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get predictions from a model\n",
    "# def get_predictions(model, dataloader):\n",
    "#     trainer = pl.Trainer(accelerator='mps', devices=-1)\n",
    "#     test_results = trainer.test(model, dataloaders=dataloader)\n",
    "    \n",
    "#     # Extract predictions from the test results\n",
    "#     preds = torch.cat([x['preds'] for x in test_results])\n",
    "#     return preds.cpu().numpy()\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_predictions(model, dataloader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    device = next(model.parameters()).device  # Get the device of the model\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        # Wrap the dataloader with tqdm for a progress bar\n",
    "        for batch in tqdm(dataloader, desc=\"Processing batches\", leave=False):\n",
    "            inputs = batch[0].to(device)  # Move inputs to the correct device\n",
    "            outputs = model(inputs)  # Forward pass\n",
    "            _, preds = torch.max(outputs, 1)  # Get predicted class indices\n",
    "            all_preds.append(preds.cpu())  # Move predictions to CPU and store\n",
    "\n",
    "    # Concatenate all predictions into a single tensor\n",
    "    return torch.cat(all_preds).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Architectures:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\u001b[AGPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/damirnurtdinov/miniconda3/envs/py39/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "/Users/damirnurtdinov/miniconda3/envs/py39/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0:   4%|▍         | 3/79 [00:48<20:31,  0.06it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from scipy.stats import mode\n",
    "architectures_list = [ 'CNN'] # TODO: check for ViT\n",
    "losses_list = ['N','B','CE']\n",
    "seeds = [42, 0, 17, 9, 3, 16, 2]\n",
    "\n",
    "\n",
    "# Function to load and test a model using PyTorch Lightning\n",
    "def load_and_test_model(arch, loss, seed, hparams, test_dataloader):\n",
    "    # Use glob to find all matching checkpoint files\n",
    "    pattern = f\"checkpoints/model-{arch}-{loss}-seed-{seed}-epoch=*.ckpt\"\n",
    "    matching_files = glob.glob(pattern)\n",
    "    \n",
    "    if not matching_files:\n",
    "        raise FileNotFoundError(f\"No checkpoint found for {arch}, {loss}, seed {seed}\")\n",
    "    \n",
    "    # Load the first matching file (or choose the latest epoch if needed)\n",
    "    model_path = matching_files[0]\n",
    "    \n",
    "    # Get the architecture and loss function based on hparams\n",
    "    arch, loss_fn = get_arch_and_loss(hparams)\n",
    "    \n",
    "    # Load the model from the checkpoint\n",
    "    best_model = train_model.load_from_checkpoint(model_path, model=arch, loss=loss_fn)\n",
    "    best_model = best_model.to(device)\n",
    "    \n",
    "    # Test the model using PyTorch Lightning's Trainer\n",
    "    trainer = pl.Trainer(accelerator='mps', devices=-1)\n",
    "    test_results = trainer.test(best_model, dataloaders=test_dataloader)\n",
    "    \n",
    "    # Extract predictions from the test results\n",
    "    preds = torch.cat([x['preds'] for x in test_results])\n",
    "    return preds.cpu().numpy()\n",
    "\n",
    "# Dictionary to store individual accuracies for each architecture, loss, and seed\n",
    "individual_accuracies = {arch: {loss: [] for loss in losses_list} for arch in architectures_list}\n",
    "\n",
    "# Dictionary to store predictions for each architecture\n",
    "architecture_predictions = {arch: [] for arch in architectures_list}\n",
    "\n",
    "# Loop through all models and collect predictions\n",
    "for arch in tqdm(architectures_list, desc=\"Architectures\"):\n",
    "    for loss in tqdm(losses_list, desc=\"Losses\", leave=False):\n",
    "        for seed in tqdm(seeds, desc=\"Seeds\", leave=False):\n",
    "            try:\n",
    "                # Update hparams for the current architecture and loss\n",
    "                hparams['architecture'] = arch\n",
    "                hparams['criterion'] = loss\n",
    "                \n",
    "                # Load and test the model\n",
    "                predictions = load_and_test_model(arch, loss, seed, hparams, test_dataloader)\n",
    "                architecture_predictions[arch].append(predictions)\n",
    "                \n",
    "                # Calculate accuracy for the current model\n",
    "                true_labels = np.array(data_module.cifar10_test.targets)\n",
    "                accuracy = accuracy_score(true_labels, predictions)\n",
    "                individual_accuracies[arch][loss].append(accuracy)\n",
    "                print(f'{arch} {loss} seed {seed} Accuracy: {accuracy:.4f}')\n",
    "            except FileNotFoundError as e:\n",
    "                print(e)\n",
    "                continue\n",
    "\n",
    "# Compute mean accuracy and standard deviation for each architecture and loss\n",
    "for arch in architectures_list:\n",
    "    for loss in losses_list:\n",
    "        accuracies = individual_accuracies[arch][loss]\n",
    "        if accuracies:  # Check if there are any accuracies for this combination\n",
    "            mean_accuracy = np.mean(accuracies)\n",
    "            std_accuracy = np.std(accuracies)\n",
    "            print(f'{arch} {loss} Mean Accuracy: {mean_accuracy:.4f} ± {std_accuracy:.4f}')\n",
    "\n",
    "# Compute ensemble accuracy for each architecture\n",
    "for arch in architectures_list:\n",
    "    if architecture_predictions[arch]:  # Check if there are any predictions for this architecture\n",
    "        all_predictions = np.stack(architecture_predictions[arch])  # Shape: (num_models, num_samples)\n",
    "        \n",
    "        # Ensemble predictions (e.g., by majority voting)\n",
    "        ensemble_predictions, _ = mode(all_predictions, axis=0)  # Majority voting\n",
    "        ensemble_predictions = ensemble_predictions.flatten()  # Flatten to 1D array\n",
    "\n",
    "        # Get true labels from the CIFAR-10 dataset\n",
    "        test_labels = np.array(data_module.cifar10_test.targets)\n",
    "\n",
    "        # Calculate ensemble accuracy\n",
    "        accuracy = accuracy_score(test_labels, ensemble_predictions)\n",
    "        print(f'{arch} Ensemble Accuracy: {accuracy:.4f}')\n",
    "\n",
    "        # Compute confusion matrix\n",
    "        cm = confusion_matrix(test_labels, ensemble_predictions)\n",
    "        print(f'{arch} Confusion Matrix:\\n{cm}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
