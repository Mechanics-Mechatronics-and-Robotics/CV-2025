{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **LAB 2 -** Computer Vision. Convolutional Neural Networks.\n",
        "Group: M24-RO-01 / M24-RO15-01\n",
        "\n",
        "Instructor: Alexey Kornaev\n",
        "\n",
        "TA: Kirill Yakovlev"
      ],
      "metadata": {
        "id": "94Wo_jSw-5BI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**INTRODUCTION**\n",
        "\n",
        "**Today we start talking about one of the most important and fundamental concept in Computer Vision - convolutions. Convolution is an operation used to extract features from data (usually high-dimensional). The operation itself simply takes a matrix made of numbers, moves it through the data, and takes the sum of products between the data and that matrix. This matrix is called kernel or filter.**"
      ],
      "metadata": {
        "id": "UQhP6eFmKYnd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let's take a simple classification case for a better understading using a pytorch framework.**"
      ],
      "metadata": {
        "id": "kvX7cYkwqVHM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "B2Il-BRogP-V"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms, models"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let's start with sequential transformations. Do you know what they are exactly doing?**"
      ],
      "metadata": {
        "id": "QwaXyJuQGROs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform_cifar_train = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "transform_cifar_test = transform_cifar_train = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "# Uploading data and finalizing dataset preparation\n",
        "\n",
        "cifar10_train = datasets.CIFAR10(root='data', train=True, transform=transform_cifar_train, download=True)\n",
        "cifar10_test = datasets.CIFAR10(root='data', train=False, transform=transform_cifar_test, download=True)\n",
        "\n",
        "cifar10_loader_train = DataLoader(cifar10_train, batch_size=32, shuffle=True)\n",
        "cifar10_loader_test = DataLoader(cifar10_test, batch_size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8oXbcGGEFiOb",
        "outputId": "5b196e06-fac9-40ea-af27-587a4868b7c0"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let's start with designing our first architecture. We want to keep it as simple as possible in the first attempt gradually improving it in next sections. This CNN consists of one convolutional layer following by the max pooling layer and finalizing our model by the fully connected layer for the classification purpose.**"
      ],
      "metadata": {
        "id": "L8K8Da9EFgt0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class My_First_CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc = nn.Linear(16 * 16 * 16, 10)\n",
        "        self.out = nn.LogSoftmax(dim=1)\n",
        "        # self.out = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.relu(self.conv(x)))\n",
        "        x = x.view(-1, 16 * 16 * 16)\n",
        "        x = self.fc(x)\n",
        "        x = self.out(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "lPBe_OeCqFo_"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let's take a look on our model!**"
      ],
      "metadata": {
        "id": "cGHdaXU4Xwe2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = My_First_CNN()\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKyT3NXMEb94",
        "outputId": "1aed7308-0555-4def-eabb-419e7c5b151b"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My_First_CNN(\n",
            "  (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc): Linear(in_features=4096, out_features=10, bias=True)\n",
            "  (out): LogSoftmax(dim=1)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Also we can check our model out and analyze its properties using summary. Sometimes it also can help you arrange size of your layers and avoid training errors.**"
      ],
      "metadata": {
        "id": "j_6KzCK9HOL7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary\n",
        "\n",
        "model = My_First_CNN()\n",
        "summary(model, input_size = (3,32,32))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ch_lEQt1HQGn",
        "outputId": "1b9347ef-4c35-445b-9c4c-f38d1e3f9b0d"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 16, 32, 32]             448\n",
            "         MaxPool2d-2           [-1, 16, 16, 16]               0\n",
            "            Linear-3                   [-1, 10]          40,970\n",
            "        LogSoftmax-4                   [-1, 10]               0\n",
            "================================================================\n",
            "Total params: 41,418\n",
            "Trainable params: 41,418\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 0.16\n",
            "Params size (MB): 0.16\n",
            "Estimated Total Size (MB): 0.33\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Next let's try to train our initial model and see what result we will get. Let's make a specific function for that so we can reuse it for further experiments.**"
      ],
      "metadata": {
        "id": "uApMt7GIZiDk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, test_loader, optimizer, criterion = torch.nn.NLLLoss(),\n",
        "          n_epochs = 10, max_epochs_stop = 3, save_file = 'model-cifar.pt'):\n",
        "\n",
        "    # specify loss function\n",
        "    criterion = criterion\n",
        "\n",
        "    # specify optimizer\n",
        "    optimizer = optimizer\n",
        "\n",
        "    epochs_no_improve = 0\n",
        "    max_epochs_stop = max_epochs_stop\n",
        "    test_loss_min = np.Inf\n",
        "\n",
        "    for epoch in range(1, n_epochs+1):\n",
        "\n",
        "        # keep track of training and Test loss\n",
        "        train_loss = 0.0\n",
        "        test_loss = 0.0\n",
        "\n",
        "        train_acc = 0\n",
        "        test_acc = 0\n",
        "\n",
        "\n",
        "\n",
        "        # TRAIN STEP\n",
        "\n",
        "\n",
        "        model.train()\n",
        "\n",
        "        for i, (data, target) in enumerate(train_loader):\n",
        "            # move tensors to GPU if CUDA is available\n",
        "            if train_on_gpu:\n",
        "                data, target = data.cuda(), target.cuda()\n",
        "            # clear the gradients of all optimized variables\n",
        "            optimizer.zero_grad()\n",
        "            # forward pass: compute predicted outputs by passing inputs to the model\n",
        "            output = model(data)\n",
        "            # calculate the batch loss\n",
        "            loss = criterion(output, target)\n",
        "            # backward pass: compute gradient of the loss with respect to model parameters\n",
        "            loss.backward()\n",
        "            # perform a single optimization step (parameter update)\n",
        "            optimizer.step()\n",
        "            # update training loss\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            # Calculate accuracy (don't forget to modify if you are using Cross-Enthropy and Softmax function)\n",
        "            ps = torch.exp(output)\n",
        "            topk, topclass = ps.topk(1, dim = 1)\n",
        "            equals = topclass == target.view(*topclass.shape)\n",
        "            accuracy = torch.mean(equals.type(torch.FloatTensor))\n",
        "            train_acc += accuracy.item()\n",
        "\n",
        "            print(f'Epoch: {epoch} \\t {100 * i / len(train_loader):.2f}% complete.', end = '\\r')\n",
        "\n",
        "        # VALIDATION STEP\n",
        "\n",
        "\n",
        "        model.eval()\n",
        "        for data, target in test_loader:\n",
        "            # move tensors to GPU if CUDA is available\n",
        "            if train_on_gpu:\n",
        "                data, target = data.cuda(), target.cuda()\n",
        "            # forward pass: compute predicted outputs by passing inputs to the model\n",
        "            output = model(data)\n",
        "            # calculate the batch loss\n",
        "            loss = criterion(output, target)\n",
        "            # update average Test loss\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            # Calculate accuracy\n",
        "            ps = torch.exp(output)\n",
        "            topk, topclass = ps.topk(1, dim = 1)\n",
        "            equals = topclass == target.view(*topclass.shape)\n",
        "            accuracy = torch.mean(equals.type(torch.FloatTensor))\n",
        "            test_acc += accuracy.item()\n",
        "\n",
        "        # calculate average losses\n",
        "        train_loss = train_loss/len(train_loader)\n",
        "        test_loss = test_loss/len(test_loader)\n",
        "\n",
        "        train_acc = train_acc/len(train_loader)\n",
        "        test_acc = test_acc/len(test_loader)\n",
        "\n",
        "        # print training/Test statistics\n",
        "        print('\\nEpoch: {} \\tTraining Loss: {:.6f} \\tTest Loss: {:.6f}'.format(\n",
        "            epoch, train_loss, test_loss))\n",
        "        print(f'Training Accuracy: {100 * train_acc:.2f}%\\t Test Accuracy: {100 * test_acc:.2f}%')\n",
        "\n",
        "        # save model if Test loss has decreased\n",
        "        if test_loss <= test_loss_min:\n",
        "            print('Test loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
        "            test_loss_min,\n",
        "            test_loss))\n",
        "            torch.save(model.state_dict(), save_file)\n",
        "            epochs_no_improve = 0\n",
        "            test_loss_min = test_loss\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            print(f'{epochs_no_improve} epochs with no improvement.')\n",
        "            if epochs_no_improve >= max_epochs_stop:\n",
        "                print('Early Stopping')\n",
        "                break"
      ],
      "metadata": {
        "id": "aNNOB35FZpuS"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let's specify our training parameters like number of training epochs, optimzier, etc. It's up to you what you think can be specified to enhance your results**"
      ],
      "metadata": {
        "id": "4aIP4rxteRuM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 20 # you may increase this number to train in a final model\n",
        "optimizer = optim.Adam(model.parameters()) # Choosing optimizer. Let's choose classical Adam optimizer\n",
        "save_file_name = 'model-cifar.pt' # define name to save weights of our model"
      ],
      "metadata": {
        "id": "aPn-0IVdgwEn"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Last step we can specify whether we want to train our model by CPU or GPU. In colab you can choose T4 in your Runtime type settings, but usage time is a strictly limited.**"
      ],
      "metadata": {
        "id": "c9CaPt-Egw1D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "if train_on_gpu:\n",
        "    model.cuda()"
      ],
      "metadata": {
        "id": "GR9M8srpg5Aa"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Start training...**"
      ],
      "metadata": {
        "id": "PB4pzkawltLS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train(model, cifar10_loader_train, cifar10_loader_test, optimizer = optimizer,\n",
        "      n_epochs = n_epochs, save_file = save_file_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tq6ehhhMb4Ri",
        "outputId": "2e976ef6-0a81-41b3-d226-dec3e6ac1a40"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 1 \tTraining Loss: 1.449822 \tTest Loss: 1.277729\n",
            "Training Accuracy: 49.13%\t Test Accuracy: 56.01%\n",
            "Test loss decreased (inf --> 1.277729).  Saving model ...\n",
            "\n",
            "Epoch: 2 \tTraining Loss: 1.193094 \tTest Loss: 1.226409\n",
            "Training Accuracy: 58.57%\t Test Accuracy: 57.04%\n",
            "Test loss decreased (1.277729 --> 1.226409).  Saving model ...\n",
            "\n",
            "Epoch: 3 \tTraining Loss: 1.082447 \tTest Loss: 1.094553\n",
            "Training Accuracy: 62.26%\t Test Accuracy: 62.00%\n",
            "Test loss decreased (1.226409 --> 1.094553).  Saving model ...\n",
            "\n",
            "Epoch: 4 \tTraining Loss: 1.006337 \tTest Loss: 1.077619\n",
            "Training Accuracy: 65.33%\t Test Accuracy: 62.81%\n",
            "Test loss decreased (1.094553 --> 1.077619).  Saving model ...\n",
            "\n",
            "Epoch: 5 \tTraining Loss: 0.958029 \tTest Loss: 1.059042\n",
            "Training Accuracy: 66.81%\t Test Accuracy: 63.65%\n",
            "Test loss decreased (1.077619 --> 1.059042).  Saving model ...\n",
            "\n",
            "Epoch: 6 \tTraining Loss: 0.924292 \tTest Loss: 1.038461\n",
            "Training Accuracy: 68.09%\t Test Accuracy: 64.78%\n",
            "Test loss decreased (1.059042 --> 1.038461).  Saving model ...\n",
            "\n",
            "Epoch: 7 \tTraining Loss: 0.895606 \tTest Loss: 1.066649\n",
            "Training Accuracy: 69.11%\t Test Accuracy: 63.86%\n",
            "1 epochs with no improvement.\n",
            "\n",
            "Epoch: 8 \tTraining Loss: 0.870724 \tTest Loss: 1.044449\n",
            "Training Accuracy: 70.13%\t Test Accuracy: 63.96%\n",
            "2 epochs with no improvement.\n",
            "\n",
            "Epoch: 9 \tTraining Loss: 0.851331 \tTest Loss: 1.063083\n",
            "Training Accuracy: 70.70%\t Test Accuracy: 64.43%\n",
            "3 epochs with no improvement.\n",
            "Early Stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**By such trivial architecture we are still able to achieve accuracy ~64.8% which is pretty decent given number of classes.**\n",
        "\n",
        "**But let's switch to something more advanced like DNN (Deep Neural Networks). Basically, it means we start stacking many layers sequentially making our model more complex as well as increasing it is ability to catch more advanced relations between training data and classes accordingly.**\n",
        "\n",
        "**TO make it more convenient pytorch has a special method [Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html) making it easier to construct more complex architectures. However, it requires some different reorganization in code from us. We can arrange this architecture as blocks for feature extraction and classification procedures.**"
      ],
      "metadata": {
        "id": "4fRUi6YYo3-r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN_advanced(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.feature_extraction_block = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(32, 16, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        self.classification_block = nn.Sequential(\n",
        "            nn.Linear(in_features= 4 * 4 * 4 * 4, out_features=10),\n",
        "        )\n",
        "        self.out = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.feature_extraction_block(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.classification_block(x)\n",
        "        x = self.out(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "o_HSa-8Wtfo2"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CNN_advanced()\n",
        "summary(model, input_size = (3,32,32))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RW2qtwwnwBch",
        "outputId": "e336eda2-c4bd-4bfa-8589-ff079b5b4544"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 16, 32, 32]             448\n",
            "              ReLU-2           [-1, 16, 32, 32]               0\n",
            "         MaxPool2d-3           [-1, 16, 16, 16]               0\n",
            "            Conv2d-4           [-1, 32, 16, 16]           4,640\n",
            "              ReLU-5           [-1, 32, 16, 16]               0\n",
            "         MaxPool2d-6             [-1, 32, 8, 8]               0\n",
            "            Conv2d-7             [-1, 16, 8, 8]           4,624\n",
            "              ReLU-8             [-1, 16, 8, 8]               0\n",
            "         MaxPool2d-9             [-1, 16, 4, 4]               0\n",
            "          Flatten-10                  [-1, 256]               0\n",
            "           Linear-11                   [-1, 10]           2,570\n",
            "       LogSoftmax-12                   [-1, 10]               0\n",
            "================================================================\n",
            "Total params: 12,282\n",
            "Trainable params: 12,282\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 0.44\n",
            "Params size (MB): 0.05\n",
            "Estimated Total Size (MB): 0.50\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We can notice that adding new convolution layers is not associated with significant parameter increase, which is crucial when you are dealing with calculation resource limitations.**\n",
        "\n",
        "**Let's try to train our new model...**\n",
        "\n",
        "**Don't hesitate to repeat training process couple (several) times, remember that we are dealing with a LOCALLY optimal parametric system that sometimes is associated with unstable results.**"
      ],
      "metadata": {
        "id": "Jclt-6BUIQw6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 30\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "save_file_name = 'model-cifar-advanced.pt'\n",
        "\n",
        "train(model, cifar10_loader_train, cifar10_loader_test, optimizer = optimizer,\n",
        "      n_epochs = n_epochs, save_file = save_file_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h4B0-ulvwQmj",
        "outputId": "fe128ad2-aa01-4442-a115-0654a3889290"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 1 \tTraining Loss: 1.525545 \tTest Loss: 1.336467\n",
            "Training Accuracy: 44.65%\t Test Accuracy: 52.57%\n",
            "Test loss decreased (inf --> 1.336467).  Saving model ...\n",
            "\n",
            "Epoch: 2 \tTraining Loss: 1.232501 \tTest Loss: 1.162223\n",
            "Training Accuracy: 56.21%\t Test Accuracy: 59.50%\n",
            "Test loss decreased (1.336467 --> 1.162223).  Saving model ...\n",
            "\n",
            "Epoch: 3 \tTraining Loss: 1.107738 \tTest Loss: 1.054220\n",
            "Training Accuracy: 61.15%\t Test Accuracy: 62.54%\n",
            "Test loss decreased (1.162223 --> 1.054220).  Saving model ...\n",
            "\n",
            "Epoch: 4 \tTraining Loss: 1.025046 \tTest Loss: 1.004530\n",
            "Training Accuracy: 64.08%\t Test Accuracy: 64.75%\n",
            "Test loss decreased (1.054220 --> 1.004530).  Saving model ...\n",
            "\n",
            "Epoch: 5 \tTraining Loss: 0.970392 \tTest Loss: 0.981290\n",
            "Training Accuracy: 65.97%\t Test Accuracy: 65.92%\n",
            "Test loss decreased (1.004530 --> 0.981290).  Saving model ...\n",
            "\n",
            "Epoch: 6 \tTraining Loss: 0.926637 \tTest Loss: 0.952419\n",
            "Training Accuracy: 67.56%\t Test Accuracy: 66.80%\n",
            "Test loss decreased (0.981290 --> 0.952419).  Saving model ...\n",
            "\n",
            "Epoch: 7 \tTraining Loss: 0.891841 \tTest Loss: 0.909957\n",
            "Training Accuracy: 68.77%\t Test Accuracy: 68.36%\n",
            "Test loss decreased (0.952419 --> 0.909957).  Saving model ...\n",
            "\n",
            "Epoch: 8 \tTraining Loss: 0.866437 \tTest Loss: 0.917880\n",
            "Training Accuracy: 69.64%\t Test Accuracy: 68.05%\n",
            "1 epochs with no improvement.\n",
            "\n",
            "Epoch: 9 \tTraining Loss: 0.840234 \tTest Loss: 0.915627\n",
            "Training Accuracy: 70.57%\t Test Accuracy: 68.50%\n",
            "2 epochs with no improvement.\n",
            "\n",
            "Epoch: 10 \tTraining Loss: 0.821176 \tTest Loss: 0.879984\n",
            "Training Accuracy: 71.48%\t Test Accuracy: 69.50%\n",
            "Test loss decreased (0.909957 --> 0.879984).  Saving model ...\n",
            "\n",
            "Epoch: 11 \tTraining Loss: 0.802791 \tTest Loss: 0.881865\n",
            "Training Accuracy: 72.08%\t Test Accuracy: 69.40%\n",
            "1 epochs with no improvement.\n",
            "\n",
            "Epoch: 12 \tTraining Loss: 0.788782 \tTest Loss: 0.891031\n",
            "Training Accuracy: 72.39%\t Test Accuracy: 69.44%\n",
            "2 epochs with no improvement.\n",
            "\n",
            "Epoch: 13 \tTraining Loss: 0.772970 \tTest Loss: 0.893994\n",
            "Training Accuracy: 72.93%\t Test Accuracy: 69.64%\n",
            "3 epochs with no improvement.\n",
            "Early Stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**As we can notice adding couple new convolution layers gave us ~4.9% increase in accuracy, which is not that bad.**\n",
        "\n",
        "**This principle with a consequent applying convolution with ReLU and max pooling can be met in advanced deep architectures like VGG16 (image below). But for CIFAR10 this architecture looks exorbitantly advanced.**\n",
        "\n",
        "![](https://www.researchgate.net/profile/Jose-Cano-6/publication/327070011/figure/fig1/AS:660549306159105@1534498635256/VGG-16-neural-network-architecture.png)\n",
        "\n",
        "**Nevertheless, for real-world problems when we usually stick to already existent solutions and architectures. Pytorch provides already pretrained models for classification tasks usually pretrained on [Image Net](https://www.image-net.org/) dataset. As of today they have the following models available:**\n",
        "\n",
        "\n",
        "\n",
        "* AlexNet\n",
        "* ConvNeXt\n",
        "* DenseNet\n",
        "* EfficientNet\n",
        "* EfficientNetV2\n",
        "* GoogLeNet\n",
        "* Inception V3\n",
        "* MaxVit\n",
        "* MNASNet\n",
        "* MobileNet V2\n",
        "* MobileNet V3\n",
        "* RegNet\n",
        "* ResNet\n",
        "* ResNeXt\n",
        "* ShuffleNet V2\n",
        "* SqueezeNet\n",
        "* SwinTransformer\n",
        "* VGG\n",
        "* VisionTransformer\n",
        "* Wide ResNets\n",
        "\n",
        "**Let's take something pretty straightforward from the architectural perspective. For example DenseNet looks appropriate for demonstrative goals. You can apply two different strategies:**\n",
        "\n",
        "1.   pretrained=**True**: use the pre-trained weights of the model which are trained on a larger database (recommended)\n",
        "2.   pretrained=**False**: begin with randomized weights and the densenet121 architecture.\n",
        "\n",
        "**Let's upload a model with pretraiend weights and check its architectural shape.**"
      ],
      "metadata": {
        "id": "xCzEcOJgICBU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.densenet121(pretrained=True)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fCDZSrcrGNaC",
        "outputId": "efb0aaa1-eda9-4a52-d64b-5a4c294a17bf"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DenseNet(\n",
            "  (features): Sequential(\n",
            "    (conv0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "    (norm0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu0): ReLU(inplace=True)\n",
            "    (pool0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    (denseblock1): _DenseBlock(\n",
            "      (denselayer1): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer2): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer3): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer4): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer5): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer6): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "    )\n",
            "    (transition1): _Transition(\n",
            "      (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
            "    )\n",
            "    (denseblock2): _DenseBlock(\n",
            "      (denselayer1): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer2): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer3): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer4): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer5): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer6): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer7): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer8): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer9): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer10): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer11): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer12): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "    )\n",
            "    (transition2): _Transition(\n",
            "      (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
            "    )\n",
            "    (denseblock3): _DenseBlock(\n",
            "      (denselayer1): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer2): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer3): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer4): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer5): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer6): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer7): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer8): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer9): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer10): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer11): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer12): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer13): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer14): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer15): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer16): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer17): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer18): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer19): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer20): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer21): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer22): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer23): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer24): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "    )\n",
            "    (transition3): _Transition(\n",
            "      (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
            "    )\n",
            "    (denseblock4): _DenseBlock(\n",
            "      (denselayer1): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer2): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer3): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer4): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer5): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer6): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer7): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer8): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer9): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer10): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer11): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer12): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer13): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer14): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer15): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer16): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "    )\n",
            "    (norm5): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (classifier): Linear(in_features=1024, out_features=1000, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**As the next step we can define some transformations step for our data before training our architectrue. It is up to you which transformations you think seems relevant.**"
      ],
      "metadata": {
        "id": "YupuinwsL8Ab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform_cifar_train_densenet = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ])\n",
        "\n",
        "transform_cifar_test_densenet = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ])"
      ],
      "metadata": {
        "id": "TxzMS8-JTTht"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cifar10_train_pre = datasets.CIFAR10(root='data', train=True, transform=transform_cifar_train_densenet, download=True)\n",
        "cifar10_test_pre = datasets.CIFAR10(root='data', train=False, transform=transform_cifar_test_densenet, download=True)\n",
        "\n",
        "cifar10_loader_train_pre = DataLoader(cifar10_train_pre, batch_size=32, shuffle=True)\n",
        "cifar10_loader_test_pre = DataLoader(cifar10_test_pre, batch_size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYgEtbeqKjVp",
        "outputId": "b499cfea-bfd7-47bf-eff6-1f21ef5ea850"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Given we want to base our solution on some pretrained architecture we are not interested to retrain already existed layers in this model.**\n",
        "\n",
        "**First let's check whether layers are in \"training\" mode.**"
      ],
      "metadata": {
        "id": "V9aQ3bu1MV_L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in model.named_parameters():\n",
        "    print(name,param.requires_grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UCzqOtUA7nHX",
        "outputId": "db0189c8-5adf-413b-c6de-35e2525c29b7"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "features.conv0.weight True\n",
            "features.norm0.weight True\n",
            "features.norm0.bias True\n",
            "features.denseblock1.denselayer1.norm1.weight True\n",
            "features.denseblock1.denselayer1.norm1.bias True\n",
            "features.denseblock1.denselayer1.conv1.weight True\n",
            "features.denseblock1.denselayer1.norm2.weight True\n",
            "features.denseblock1.denselayer1.norm2.bias True\n",
            "features.denseblock1.denselayer1.conv2.weight True\n",
            "features.denseblock1.denselayer2.norm1.weight True\n",
            "features.denseblock1.denselayer2.norm1.bias True\n",
            "features.denseblock1.denselayer2.conv1.weight True\n",
            "features.denseblock1.denselayer2.norm2.weight True\n",
            "features.denseblock1.denselayer2.norm2.bias True\n",
            "features.denseblock1.denselayer2.conv2.weight True\n",
            "features.denseblock1.denselayer3.norm1.weight True\n",
            "features.denseblock1.denselayer3.norm1.bias True\n",
            "features.denseblock1.denselayer3.conv1.weight True\n",
            "features.denseblock1.denselayer3.norm2.weight True\n",
            "features.denseblock1.denselayer3.norm2.bias True\n",
            "features.denseblock1.denselayer3.conv2.weight True\n",
            "features.denseblock1.denselayer4.norm1.weight True\n",
            "features.denseblock1.denselayer4.norm1.bias True\n",
            "features.denseblock1.denselayer4.conv1.weight True\n",
            "features.denseblock1.denselayer4.norm2.weight True\n",
            "features.denseblock1.denselayer4.norm2.bias True\n",
            "features.denseblock1.denselayer4.conv2.weight True\n",
            "features.denseblock1.denselayer5.norm1.weight True\n",
            "features.denseblock1.denselayer5.norm1.bias True\n",
            "features.denseblock1.denselayer5.conv1.weight True\n",
            "features.denseblock1.denselayer5.norm2.weight True\n",
            "features.denseblock1.denselayer5.norm2.bias True\n",
            "features.denseblock1.denselayer5.conv2.weight True\n",
            "features.denseblock1.denselayer6.norm1.weight True\n",
            "features.denseblock1.denselayer6.norm1.bias True\n",
            "features.denseblock1.denselayer6.conv1.weight True\n",
            "features.denseblock1.denselayer6.norm2.weight True\n",
            "features.denseblock1.denselayer6.norm2.bias True\n",
            "features.denseblock1.denselayer6.conv2.weight True\n",
            "features.transition1.norm.weight True\n",
            "features.transition1.norm.bias True\n",
            "features.transition1.conv.weight True\n",
            "features.denseblock2.denselayer1.norm1.weight True\n",
            "features.denseblock2.denselayer1.norm1.bias True\n",
            "features.denseblock2.denselayer1.conv1.weight True\n",
            "features.denseblock2.denselayer1.norm2.weight True\n",
            "features.denseblock2.denselayer1.norm2.bias True\n",
            "features.denseblock2.denselayer1.conv2.weight True\n",
            "features.denseblock2.denselayer2.norm1.weight True\n",
            "features.denseblock2.denselayer2.norm1.bias True\n",
            "features.denseblock2.denselayer2.conv1.weight True\n",
            "features.denseblock2.denselayer2.norm2.weight True\n",
            "features.denseblock2.denselayer2.norm2.bias True\n",
            "features.denseblock2.denselayer2.conv2.weight True\n",
            "features.denseblock2.denselayer3.norm1.weight True\n",
            "features.denseblock2.denselayer3.norm1.bias True\n",
            "features.denseblock2.denselayer3.conv1.weight True\n",
            "features.denseblock2.denselayer3.norm2.weight True\n",
            "features.denseblock2.denselayer3.norm2.bias True\n",
            "features.denseblock2.denselayer3.conv2.weight True\n",
            "features.denseblock2.denselayer4.norm1.weight True\n",
            "features.denseblock2.denselayer4.norm1.bias True\n",
            "features.denseblock2.denselayer4.conv1.weight True\n",
            "features.denseblock2.denselayer4.norm2.weight True\n",
            "features.denseblock2.denselayer4.norm2.bias True\n",
            "features.denseblock2.denselayer4.conv2.weight True\n",
            "features.denseblock2.denselayer5.norm1.weight True\n",
            "features.denseblock2.denselayer5.norm1.bias True\n",
            "features.denseblock2.denselayer5.conv1.weight True\n",
            "features.denseblock2.denselayer5.norm2.weight True\n",
            "features.denseblock2.denselayer5.norm2.bias True\n",
            "features.denseblock2.denselayer5.conv2.weight True\n",
            "features.denseblock2.denselayer6.norm1.weight True\n",
            "features.denseblock2.denselayer6.norm1.bias True\n",
            "features.denseblock2.denselayer6.conv1.weight True\n",
            "features.denseblock2.denselayer6.norm2.weight True\n",
            "features.denseblock2.denselayer6.norm2.bias True\n",
            "features.denseblock2.denselayer6.conv2.weight True\n",
            "features.denseblock2.denselayer7.norm1.weight True\n",
            "features.denseblock2.denselayer7.norm1.bias True\n",
            "features.denseblock2.denselayer7.conv1.weight True\n",
            "features.denseblock2.denselayer7.norm2.weight True\n",
            "features.denseblock2.denselayer7.norm2.bias True\n",
            "features.denseblock2.denselayer7.conv2.weight True\n",
            "features.denseblock2.denselayer8.norm1.weight True\n",
            "features.denseblock2.denselayer8.norm1.bias True\n",
            "features.denseblock2.denselayer8.conv1.weight True\n",
            "features.denseblock2.denselayer8.norm2.weight True\n",
            "features.denseblock2.denselayer8.norm2.bias True\n",
            "features.denseblock2.denselayer8.conv2.weight True\n",
            "features.denseblock2.denselayer9.norm1.weight True\n",
            "features.denseblock2.denselayer9.norm1.bias True\n",
            "features.denseblock2.denselayer9.conv1.weight True\n",
            "features.denseblock2.denselayer9.norm2.weight True\n",
            "features.denseblock2.denselayer9.norm2.bias True\n",
            "features.denseblock2.denselayer9.conv2.weight True\n",
            "features.denseblock2.denselayer10.norm1.weight True\n",
            "features.denseblock2.denselayer10.norm1.bias True\n",
            "features.denseblock2.denselayer10.conv1.weight True\n",
            "features.denseblock2.denselayer10.norm2.weight True\n",
            "features.denseblock2.denselayer10.norm2.bias True\n",
            "features.denseblock2.denselayer10.conv2.weight True\n",
            "features.denseblock2.denselayer11.norm1.weight True\n",
            "features.denseblock2.denselayer11.norm1.bias True\n",
            "features.denseblock2.denselayer11.conv1.weight True\n",
            "features.denseblock2.denselayer11.norm2.weight True\n",
            "features.denseblock2.denselayer11.norm2.bias True\n",
            "features.denseblock2.denselayer11.conv2.weight True\n",
            "features.denseblock2.denselayer12.norm1.weight True\n",
            "features.denseblock2.denselayer12.norm1.bias True\n",
            "features.denseblock2.denselayer12.conv1.weight True\n",
            "features.denseblock2.denselayer12.norm2.weight True\n",
            "features.denseblock2.denselayer12.norm2.bias True\n",
            "features.denseblock2.denselayer12.conv2.weight True\n",
            "features.transition2.norm.weight True\n",
            "features.transition2.norm.bias True\n",
            "features.transition2.conv.weight True\n",
            "features.denseblock3.denselayer1.norm1.weight True\n",
            "features.denseblock3.denselayer1.norm1.bias True\n",
            "features.denseblock3.denselayer1.conv1.weight True\n",
            "features.denseblock3.denselayer1.norm2.weight True\n",
            "features.denseblock3.denselayer1.norm2.bias True\n",
            "features.denseblock3.denselayer1.conv2.weight True\n",
            "features.denseblock3.denselayer2.norm1.weight True\n",
            "features.denseblock3.denselayer2.norm1.bias True\n",
            "features.denseblock3.denselayer2.conv1.weight True\n",
            "features.denseblock3.denselayer2.norm2.weight True\n",
            "features.denseblock3.denselayer2.norm2.bias True\n",
            "features.denseblock3.denselayer2.conv2.weight True\n",
            "features.denseblock3.denselayer3.norm1.weight True\n",
            "features.denseblock3.denselayer3.norm1.bias True\n",
            "features.denseblock3.denselayer3.conv1.weight True\n",
            "features.denseblock3.denselayer3.norm2.weight True\n",
            "features.denseblock3.denselayer3.norm2.bias True\n",
            "features.denseblock3.denselayer3.conv2.weight True\n",
            "features.denseblock3.denselayer4.norm1.weight True\n",
            "features.denseblock3.denselayer4.norm1.bias True\n",
            "features.denseblock3.denselayer4.conv1.weight True\n",
            "features.denseblock3.denselayer4.norm2.weight True\n",
            "features.denseblock3.denselayer4.norm2.bias True\n",
            "features.denseblock3.denselayer4.conv2.weight True\n",
            "features.denseblock3.denselayer5.norm1.weight True\n",
            "features.denseblock3.denselayer5.norm1.bias True\n",
            "features.denseblock3.denselayer5.conv1.weight True\n",
            "features.denseblock3.denselayer5.norm2.weight True\n",
            "features.denseblock3.denselayer5.norm2.bias True\n",
            "features.denseblock3.denselayer5.conv2.weight True\n",
            "features.denseblock3.denselayer6.norm1.weight True\n",
            "features.denseblock3.denselayer6.norm1.bias True\n",
            "features.denseblock3.denselayer6.conv1.weight True\n",
            "features.denseblock3.denselayer6.norm2.weight True\n",
            "features.denseblock3.denselayer6.norm2.bias True\n",
            "features.denseblock3.denselayer6.conv2.weight True\n",
            "features.denseblock3.denselayer7.norm1.weight True\n",
            "features.denseblock3.denselayer7.norm1.bias True\n",
            "features.denseblock3.denselayer7.conv1.weight True\n",
            "features.denseblock3.denselayer7.norm2.weight True\n",
            "features.denseblock3.denselayer7.norm2.bias True\n",
            "features.denseblock3.denselayer7.conv2.weight True\n",
            "features.denseblock3.denselayer8.norm1.weight True\n",
            "features.denseblock3.denselayer8.norm1.bias True\n",
            "features.denseblock3.denselayer8.conv1.weight True\n",
            "features.denseblock3.denselayer8.norm2.weight True\n",
            "features.denseblock3.denselayer8.norm2.bias True\n",
            "features.denseblock3.denselayer8.conv2.weight True\n",
            "features.denseblock3.denselayer9.norm1.weight True\n",
            "features.denseblock3.denselayer9.norm1.bias True\n",
            "features.denseblock3.denselayer9.conv1.weight True\n",
            "features.denseblock3.denselayer9.norm2.weight True\n",
            "features.denseblock3.denselayer9.norm2.bias True\n",
            "features.denseblock3.denselayer9.conv2.weight True\n",
            "features.denseblock3.denselayer10.norm1.weight True\n",
            "features.denseblock3.denselayer10.norm1.bias True\n",
            "features.denseblock3.denselayer10.conv1.weight True\n",
            "features.denseblock3.denselayer10.norm2.weight True\n",
            "features.denseblock3.denselayer10.norm2.bias True\n",
            "features.denseblock3.denselayer10.conv2.weight True\n",
            "features.denseblock3.denselayer11.norm1.weight True\n",
            "features.denseblock3.denselayer11.norm1.bias True\n",
            "features.denseblock3.denselayer11.conv1.weight True\n",
            "features.denseblock3.denselayer11.norm2.weight True\n",
            "features.denseblock3.denselayer11.norm2.bias True\n",
            "features.denseblock3.denselayer11.conv2.weight True\n",
            "features.denseblock3.denselayer12.norm1.weight True\n",
            "features.denseblock3.denselayer12.norm1.bias True\n",
            "features.denseblock3.denselayer12.conv1.weight True\n",
            "features.denseblock3.denselayer12.norm2.weight True\n",
            "features.denseblock3.denselayer12.norm2.bias True\n",
            "features.denseblock3.denselayer12.conv2.weight True\n",
            "features.denseblock3.denselayer13.norm1.weight True\n",
            "features.denseblock3.denselayer13.norm1.bias True\n",
            "features.denseblock3.denselayer13.conv1.weight True\n",
            "features.denseblock3.denselayer13.norm2.weight True\n",
            "features.denseblock3.denselayer13.norm2.bias True\n",
            "features.denseblock3.denselayer13.conv2.weight True\n",
            "features.denseblock3.denselayer14.norm1.weight True\n",
            "features.denseblock3.denselayer14.norm1.bias True\n",
            "features.denseblock3.denselayer14.conv1.weight True\n",
            "features.denseblock3.denselayer14.norm2.weight True\n",
            "features.denseblock3.denselayer14.norm2.bias True\n",
            "features.denseblock3.denselayer14.conv2.weight True\n",
            "features.denseblock3.denselayer15.norm1.weight True\n",
            "features.denseblock3.denselayer15.norm1.bias True\n",
            "features.denseblock3.denselayer15.conv1.weight True\n",
            "features.denseblock3.denselayer15.norm2.weight True\n",
            "features.denseblock3.denselayer15.norm2.bias True\n",
            "features.denseblock3.denselayer15.conv2.weight True\n",
            "features.denseblock3.denselayer16.norm1.weight True\n",
            "features.denseblock3.denselayer16.norm1.bias True\n",
            "features.denseblock3.denselayer16.conv1.weight True\n",
            "features.denseblock3.denselayer16.norm2.weight True\n",
            "features.denseblock3.denselayer16.norm2.bias True\n",
            "features.denseblock3.denselayer16.conv2.weight True\n",
            "features.denseblock3.denselayer17.norm1.weight True\n",
            "features.denseblock3.denselayer17.norm1.bias True\n",
            "features.denseblock3.denselayer17.conv1.weight True\n",
            "features.denseblock3.denselayer17.norm2.weight True\n",
            "features.denseblock3.denselayer17.norm2.bias True\n",
            "features.denseblock3.denselayer17.conv2.weight True\n",
            "features.denseblock3.denselayer18.norm1.weight True\n",
            "features.denseblock3.denselayer18.norm1.bias True\n",
            "features.denseblock3.denselayer18.conv1.weight True\n",
            "features.denseblock3.denselayer18.norm2.weight True\n",
            "features.denseblock3.denselayer18.norm2.bias True\n",
            "features.denseblock3.denselayer18.conv2.weight True\n",
            "features.denseblock3.denselayer19.norm1.weight True\n",
            "features.denseblock3.denselayer19.norm1.bias True\n",
            "features.denseblock3.denselayer19.conv1.weight True\n",
            "features.denseblock3.denselayer19.norm2.weight True\n",
            "features.denseblock3.denselayer19.norm2.bias True\n",
            "features.denseblock3.denselayer19.conv2.weight True\n",
            "features.denseblock3.denselayer20.norm1.weight True\n",
            "features.denseblock3.denselayer20.norm1.bias True\n",
            "features.denseblock3.denselayer20.conv1.weight True\n",
            "features.denseblock3.denselayer20.norm2.weight True\n",
            "features.denseblock3.denselayer20.norm2.bias True\n",
            "features.denseblock3.denselayer20.conv2.weight True\n",
            "features.denseblock3.denselayer21.norm1.weight True\n",
            "features.denseblock3.denselayer21.norm1.bias True\n",
            "features.denseblock3.denselayer21.conv1.weight True\n",
            "features.denseblock3.denselayer21.norm2.weight True\n",
            "features.denseblock3.denselayer21.norm2.bias True\n",
            "features.denseblock3.denselayer21.conv2.weight True\n",
            "features.denseblock3.denselayer22.norm1.weight True\n",
            "features.denseblock3.denselayer22.norm1.bias True\n",
            "features.denseblock3.denselayer22.conv1.weight True\n",
            "features.denseblock3.denselayer22.norm2.weight True\n",
            "features.denseblock3.denselayer22.norm2.bias True\n",
            "features.denseblock3.denselayer22.conv2.weight True\n",
            "features.denseblock3.denselayer23.norm1.weight True\n",
            "features.denseblock3.denselayer23.norm1.bias True\n",
            "features.denseblock3.denselayer23.conv1.weight True\n",
            "features.denseblock3.denselayer23.norm2.weight True\n",
            "features.denseblock3.denselayer23.norm2.bias True\n",
            "features.denseblock3.denselayer23.conv2.weight True\n",
            "features.denseblock3.denselayer24.norm1.weight True\n",
            "features.denseblock3.denselayer24.norm1.bias True\n",
            "features.denseblock3.denselayer24.conv1.weight True\n",
            "features.denseblock3.denselayer24.norm2.weight True\n",
            "features.denseblock3.denselayer24.norm2.bias True\n",
            "features.denseblock3.denselayer24.conv2.weight True\n",
            "features.transition3.norm.weight True\n",
            "features.transition3.norm.bias True\n",
            "features.transition3.conv.weight True\n",
            "features.denseblock4.denselayer1.norm1.weight True\n",
            "features.denseblock4.denselayer1.norm1.bias True\n",
            "features.denseblock4.denselayer1.conv1.weight True\n",
            "features.denseblock4.denselayer1.norm2.weight True\n",
            "features.denseblock4.denselayer1.norm2.bias True\n",
            "features.denseblock4.denselayer1.conv2.weight True\n",
            "features.denseblock4.denselayer2.norm1.weight True\n",
            "features.denseblock4.denselayer2.norm1.bias True\n",
            "features.denseblock4.denselayer2.conv1.weight True\n",
            "features.denseblock4.denselayer2.norm2.weight True\n",
            "features.denseblock4.denselayer2.norm2.bias True\n",
            "features.denseblock4.denselayer2.conv2.weight True\n",
            "features.denseblock4.denselayer3.norm1.weight True\n",
            "features.denseblock4.denselayer3.norm1.bias True\n",
            "features.denseblock4.denselayer3.conv1.weight True\n",
            "features.denseblock4.denselayer3.norm2.weight True\n",
            "features.denseblock4.denselayer3.norm2.bias True\n",
            "features.denseblock4.denselayer3.conv2.weight True\n",
            "features.denseblock4.denselayer4.norm1.weight True\n",
            "features.denseblock4.denselayer4.norm1.bias True\n",
            "features.denseblock4.denselayer4.conv1.weight True\n",
            "features.denseblock4.denselayer4.norm2.weight True\n",
            "features.denseblock4.denselayer4.norm2.bias True\n",
            "features.denseblock4.denselayer4.conv2.weight True\n",
            "features.denseblock4.denselayer5.norm1.weight True\n",
            "features.denseblock4.denselayer5.norm1.bias True\n",
            "features.denseblock4.denselayer5.conv1.weight True\n",
            "features.denseblock4.denselayer5.norm2.weight True\n",
            "features.denseblock4.denselayer5.norm2.bias True\n",
            "features.denseblock4.denselayer5.conv2.weight True\n",
            "features.denseblock4.denselayer6.norm1.weight True\n",
            "features.denseblock4.denselayer6.norm1.bias True\n",
            "features.denseblock4.denselayer6.conv1.weight True\n",
            "features.denseblock4.denselayer6.norm2.weight True\n",
            "features.denseblock4.denselayer6.norm2.bias True\n",
            "features.denseblock4.denselayer6.conv2.weight True\n",
            "features.denseblock4.denselayer7.norm1.weight True\n",
            "features.denseblock4.denselayer7.norm1.bias True\n",
            "features.denseblock4.denselayer7.conv1.weight True\n",
            "features.denseblock4.denselayer7.norm2.weight True\n",
            "features.denseblock4.denselayer7.norm2.bias True\n",
            "features.denseblock4.denselayer7.conv2.weight True\n",
            "features.denseblock4.denselayer8.norm1.weight True\n",
            "features.denseblock4.denselayer8.norm1.bias True\n",
            "features.denseblock4.denselayer8.conv1.weight True\n",
            "features.denseblock4.denselayer8.norm2.weight True\n",
            "features.denseblock4.denselayer8.norm2.bias True\n",
            "features.denseblock4.denselayer8.conv2.weight True\n",
            "features.denseblock4.denselayer9.norm1.weight True\n",
            "features.denseblock4.denselayer9.norm1.bias True\n",
            "features.denseblock4.denselayer9.conv1.weight True\n",
            "features.denseblock4.denselayer9.norm2.weight True\n",
            "features.denseblock4.denselayer9.norm2.bias True\n",
            "features.denseblock4.denselayer9.conv2.weight True\n",
            "features.denseblock4.denselayer10.norm1.weight True\n",
            "features.denseblock4.denselayer10.norm1.bias True\n",
            "features.denseblock4.denselayer10.conv1.weight True\n",
            "features.denseblock4.denselayer10.norm2.weight True\n",
            "features.denseblock4.denselayer10.norm2.bias True\n",
            "features.denseblock4.denselayer10.conv2.weight True\n",
            "features.denseblock4.denselayer11.norm1.weight True\n",
            "features.denseblock4.denselayer11.norm1.bias True\n",
            "features.denseblock4.denselayer11.conv1.weight True\n",
            "features.denseblock4.denselayer11.norm2.weight True\n",
            "features.denseblock4.denselayer11.norm2.bias True\n",
            "features.denseblock4.denselayer11.conv2.weight True\n",
            "features.denseblock4.denselayer12.norm1.weight True\n",
            "features.denseblock4.denselayer12.norm1.bias True\n",
            "features.denseblock4.denselayer12.conv1.weight True\n",
            "features.denseblock4.denselayer12.norm2.weight True\n",
            "features.denseblock4.denselayer12.norm2.bias True\n",
            "features.denseblock4.denselayer12.conv2.weight True\n",
            "features.denseblock4.denselayer13.norm1.weight True\n",
            "features.denseblock4.denselayer13.norm1.bias True\n",
            "features.denseblock4.denselayer13.conv1.weight True\n",
            "features.denseblock4.denselayer13.norm2.weight True\n",
            "features.denseblock4.denselayer13.norm2.bias True\n",
            "features.denseblock4.denselayer13.conv2.weight True\n",
            "features.denseblock4.denselayer14.norm1.weight True\n",
            "features.denseblock4.denselayer14.norm1.bias True\n",
            "features.denseblock4.denselayer14.conv1.weight True\n",
            "features.denseblock4.denselayer14.norm2.weight True\n",
            "features.denseblock4.denselayer14.norm2.bias True\n",
            "features.denseblock4.denselayer14.conv2.weight True\n",
            "features.denseblock4.denselayer15.norm1.weight True\n",
            "features.denseblock4.denselayer15.norm1.bias True\n",
            "features.denseblock4.denselayer15.conv1.weight True\n",
            "features.denseblock4.denselayer15.norm2.weight True\n",
            "features.denseblock4.denselayer15.norm2.bias True\n",
            "features.denseblock4.denselayer15.conv2.weight True\n",
            "features.denseblock4.denselayer16.norm1.weight True\n",
            "features.denseblock4.denselayer16.norm1.bias True\n",
            "features.denseblock4.denselayer16.conv1.weight True\n",
            "features.denseblock4.denselayer16.norm2.weight True\n",
            "features.denseblock4.denselayer16.norm2.bias True\n",
            "features.denseblock4.denselayer16.conv2.weight True\n",
            "features.norm5.weight True\n",
            "features.norm5.bias True\n",
            "classifier.weight True\n",
            "classifier.bias True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Next let's change this by applying False mode for each layer in the pretrained model. Since we know that classifier layer is responsible specifically for distinguising classes we can replace it by some new parametric module. For this purpose we can also apply Sequential method from the previous section.**"
      ],
      "metadata": {
        "id": "9raVM8IHM9Td"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for param in model.parameters():\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "id": "wNo4xpBHR9jj"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let's take a layer for retraining...**"
      ],
      "metadata": {
        "id": "UXgFDYr14uyp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for param in model.features.denseblock4.parameters():\n",
        "    param.requires_grad = True"
      ],
      "metadata": {
        "id": "y2UB_H5R4t1n"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_features = model.classifier.in_features"
      ],
      "metadata": {
        "id": "DjZ-ugJ2Re_Y"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.classifier = nn.Sequential(\n",
        "                        nn.Linear(num_features, 256),\n",
        "                        nn.LeakyReLU(),\n",
        "                        nn.Dropout(0.3),\n",
        "                        nn.Linear(256, 10),\n",
        "                        nn.LogSoftmax(dim=1))\n",
        "\n",
        "                        # nn.Softmax(dim=1))"
      ],
      "metadata": {
        "id": "rTXIdDvPKwBf"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 40\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "save_file_name = 'model-cifar-pretrained.pt'\n",
        "\n",
        "train(model, cifar10_loader_train_pre, cifar10_loader_test_pre, optimizer = optimizer,\n",
        "      n_epochs = n_epochs, criterion = torch.nn.NLLLoss(), max_epochs_stop = 10, save_file = save_file_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WVDywuWRNMD0",
        "outputId": "27512886-81c4-4d39-f0a6-bc266a93e83e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 1 \tTraining Loss: 1.272059 \tTest Loss: 1.039625\n",
            "Training Accuracy: 55.48%\t Test Accuracy: 63.54%\n",
            "Test loss decreased (inf --> 1.039625).  Saving model ...\n",
            "\n",
            "Epoch: 2 \tTraining Loss: 1.045023 \tTest Loss: 0.984338\n",
            "Training Accuracy: 63.40%\t Test Accuracy: 65.91%\n",
            "Test loss decreased (1.039625 --> 0.984338).  Saving model ...\n",
            "\n",
            "Epoch: 3 \tTraining Loss: 0.954804 \tTest Loss: 0.971447\n",
            "Training Accuracy: 66.41%\t Test Accuracy: 66.74%\n",
            "Test loss decreased (0.984338 --> 0.971447).  Saving model ...\n",
            "\n",
            "Epoch: 4 \tTraining Loss: 0.883186 \tTest Loss: 0.968694\n",
            "Training Accuracy: 69.02%\t Test Accuracy: 67.18%\n",
            "Test loss decreased (0.971447 --> 0.968694).  Saving model ...\n",
            "\n",
            "Epoch: 5 \tTraining Loss: 0.824159 \tTest Loss: 0.944177\n",
            "Training Accuracy: 70.96%\t Test Accuracy: 68.38%\n",
            "Test loss decreased (0.968694 --> 0.944177).  Saving model ...\n",
            "\n",
            "Epoch: 6 \tTraining Loss: 0.773601 \tTest Loss: 0.927481\n",
            "Training Accuracy: 72.81%\t Test Accuracy: 68.61%\n",
            "Test loss decreased (0.944177 --> 0.927481).  Saving model ...\n",
            "\n",
            "Epoch: 7 \tTraining Loss: 0.732662 \tTest Loss: 0.944799\n",
            "Training Accuracy: 74.24%\t Test Accuracy: 68.10%\n",
            "1 epochs with no improvement.\n",
            "\n",
            "Epoch: 8 \tTraining Loss: 0.696421 \tTest Loss: 0.986708\n",
            "Training Accuracy: 75.49%\t Test Accuracy: 67.55%\n",
            "2 epochs with no improvement.\n",
            "\n",
            "Epoch: 9 \tTraining Loss: 0.658630 \tTest Loss: 0.973260\n",
            "Training Accuracy: 76.68%\t Test Accuracy: 68.44%\n",
            "3 epochs with no improvement.\n",
            "\n",
            "Epoch: 10 \tTraining Loss: 0.621870 \tTest Loss: 0.979897\n",
            "Training Accuracy: 78.10%\t Test Accuracy: 68.15%\n",
            "4 epochs with no improvement.\n",
            "\n",
            "Epoch: 11 \tTraining Loss: 0.597668 \tTest Loss: 0.959355\n",
            "Training Accuracy: 79.12%\t Test Accuracy: 69.21%\n",
            "5 epochs with no improvement.\n",
            "\n",
            "Epoch: 12 \tTraining Loss: 0.570650 \tTest Loss: 0.978911\n",
            "Training Accuracy: 79.81%\t Test Accuracy: 68.74%\n",
            "6 epochs with no improvement.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**AS we might notice result is not that drastically better compared to our initial model with convolution layers. Still you can try some new models as well as new a new shape for a classifier module.**\n",
        "\n",
        "**That's it for today...**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "AjfwHxAtN6VY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Home Task**"
      ],
      "metadata": {
        "id": "9N8Lx8eVryu0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can choose any multi-label classification dataset for the following tasks(you can still stick with CIFAR10)\n",
        "\n",
        "\n",
        "1.   In this task you need to improve the last architecture (CNN_advanced) using unused methods and tricks to improve the final result. As an example this could be:\n",
        "\n",
        "*   Batch Normalization\n",
        "*   Dropout\n",
        "*   Random Filter Pruning\n",
        "*   etc.\n",
        "\n",
        "Result should be improved compared to the initial one we have had during the lab seesion.\n",
        "2.  Train an additional architecture using any pretrained model\n",
        "\n",
        "Requirements:\n",
        "\n",
        "\n",
        "*   You should have your own classification layer (usually the last one) or layer that is responsible for classification. You can add a new layer if a classification layer is initially absent\n",
        "*   You are able to choose at least one layer to retrain from the pretrained architecture\n",
        "*   Architecture should beat the initial model in the 1st task\n",
        "*   You  can choose any architecture from the pretrained list\n",
        "\n",
        "3.  Explain what methods (or model additions) have had the biggest impact on the test set accuracy. Provide graphical comparison in metrics between models you have obtained in tasks 1 and 2\n",
        "\n",
        "\n",
        "Remember that you have approximately **3 days** to complete these tasks. You are allowed to use any avaialble computational resource.\n",
        "\n",
        "**Expected result:** uploaded Jupyter notebook with completed tasks to the Moodle assignment section."
      ],
      "metadata": {
        "id": "HB-PX6DlOh-8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# *** YOUR CODE ***"
      ],
      "metadata": {
        "id": "YPxJvBJ1OBYb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}