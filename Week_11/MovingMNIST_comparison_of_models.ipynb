{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNsdwhjzDAUYaYFCihuyzvt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mechanics-Mechatronics-and-Robotics/CV-2025/blob/main/Week_11/MovingMNIST_comparison_of_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1xPG0sxYQxL"
      },
      "outputs": [],
      "source": [
        "!pip install -q pytorch-lightning clearml"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision.transforms import ToTensor\n",
        "import torchmetrics\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.utilities import rank_zero_info\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "from pytorch_lightning.loggers import CSVLogger\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "from clearml import Task\n",
        "import random"
      ],
      "metadata": {
        "id": "fhm9kWYxZK8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Enter your code here to implement Step 2 of the logging instruction as it is shown below\n",
        "%env CLEARML_WEB_HOST=https://app.clear.ml/\n",
        "%env CLEARML_API_HOST=https://api.clear.ml\n",
        "%env CLEARML_FILES_HOST=https://files.clear.ml\n",
        "%env CLEARML_API_ACCESS_KEY=ZP02U03C6V5ER4K9VWRNZT7EWA5ZTV\n",
        "%env CLEARML_API_SECRET_KEY=BtA5GXZufr6QGpaqhX1GSKPTvaCt56OLqaNqUGLNoxx2Ye8Ctwbui0Ln5OXVnzUgH4I"
      ],
      "metadata": {
        "id": "_9SHQ7gSZMNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Configuration\n",
        "config = {\n",
        "    # Data Config\n",
        "    \"img_size\": 64,\n",
        "    \"total_frames\": 20,        # Total frames in each video\n",
        "    \"sample_frames\": {         # Frames sampled per model type\n",
        "        \"2D\": 1,\n",
        "        \"3D\": 5,\n",
        "        \"2D+1D\": 5\n",
        "    },\n",
        "    \"batch_size\": 32,\n",
        "\n",
        "    # Architecture\n",
        "    \"conv2d_channels\": [1, 32, 64],\n",
        "    \"conv3d_channels\": [1, 32, 64],\n",
        "    \"kernel_size\": (3,3,3),\n",
        "    \"pool_size\": 2,\n",
        "    \"linear_hidden\": 128,\n",
        "    \"lstm_hidden\": 128,\n",
        "\n",
        "    # Training\n",
        "    \"max_epochs\": 2,\n",
        "    \"learning_rate\": 1e-3,\n",
        "    \"early_stop_patience\": 5,\n",
        "\n",
        "    # System\n",
        "    \"accelerator\": \"auto\",\n",
        "    \"log_dir\": \"./logs\",\n",
        "    \"checkpoint_dir\": \"./checkpoints\"\n",
        "}\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    videos, targets = zip(*batch)\n",
        "\n",
        "    # Handle mixed 3D/4D inputs\n",
        "    if isinstance(videos[0], torch.Tensor):\n",
        "        if videos[0].dim() == 3:  # 2D case [C,H,W]\n",
        "            return torch.stack(videos), torch.stack(targets)\n",
        "        else:  # 3D case [T,C,H,W]\n",
        "            return torch.stack(videos), torch.stack(targets)\n",
        "    raise ValueError(\"Unexpected input dimensions\")\n",
        "\n",
        "# Enhanced MovingMNIST Dataset with frame sampling\n",
        "class MovingMNIST(Dataset):\n",
        "    def __init__(self, train=True, img_size=64, total_frames=20):\n",
        "        self.mnist = MNIST(root='./data', train=train, download=True, transform=ToTensor())\n",
        "        self.img_size = img_size\n",
        "        self.total_frames = total_frames\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.mnist) // 2\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        digit1, label1 = self.mnist[2 * idx]\n",
        "        digit2, label2 = self.mnist[2 * idx + 1]\n",
        "\n",
        "        frames = []\n",
        "        pos1 = np.random.randint(0, self.img_size - 28 + 1, size=2)\n",
        "        pos2 = np.random.randint(0, self.img_size - 28 + 1, size=2)\n",
        "        vel1 = np.random.randint(-3, 4, size=2)\n",
        "        vel2 = np.random.randint(-3, 4, size=2)\n",
        "\n",
        "        for _ in range(self.total_frames):\n",
        "            frame = torch.zeros((1, self.img_size, self.img_size))\n",
        "\n",
        "            def update_pos(pos, vel):\n",
        "                new_pos = pos + vel\n",
        "                for i in range(2):\n",
        "                    if new_pos[i] < 0:\n",
        "                        vel[i] *= -1\n",
        "                        new_pos[i] = 0\n",
        "                    elif new_pos[i] > (self.img_size - 28):\n",
        "                        vel[i] *= -1\n",
        "                        new_pos[i] = 2*(self.img_size - 28) - new_pos[i]\n",
        "                return new_pos\n",
        "\n",
        "            pos1 = update_pos(pos1, vel1)\n",
        "            pos2 = update_pos(pos2, vel2)\n",
        "\n",
        "            x1, y1 = int(pos1[0]), int(pos1[1])\n",
        "            frame[:, y1:y1+28, x1:x1+28] += digit1.squeeze()\n",
        "            x2, y2 = int(pos2[0]), int(pos2[1])\n",
        "            frame[:, y2:y2+28, x2:x2+28] += digit2.squeeze()\n",
        "\n",
        "            frames.append(frame.clamp(0, 1))\n",
        "\n",
        "        video = torch.stack(frames)\n",
        "        target = label1 + label2\n",
        "        return video, target\n",
        "\n",
        "# Frame sampling data loader\n",
        "class FrameSampler:\n",
        "    def __init__(self, dataset, model_type, sample_frames):\n",
        "        self.dataset = dataset\n",
        "        self.model_type = model_type\n",
        "        self.sample_frames = sample_frames\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        video, target = self.dataset[idx]\n",
        "\n",
        "        if self.model_type == \"2D\":\n",
        "            # For 2D CNN: return single frame without temporal dimension\n",
        "            frame_idx = random.randint(0, len(video)-1)\n",
        "            return video[frame_idx], target  # Shape [C,H,W]\n",
        "        else:\n",
        "            # For 3D/Hybrid: keep temporal dimension\n",
        "            max_start = len(video) - self.sample_frames\n",
        "            start = random.randint(0, max(0, max_start))\n",
        "            return video[start:start+self.sample_frames], target  # Shape [T,C,H,W]\n",
        "\n",
        "# Model Architectures\n",
        "class CNN2D(pl.LightningModule):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "        self.backbone = nn.Sequential(\n",
        "            nn.Conv2d(1, config[\"conv2d_channels\"][1], 3),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(config[\"pool_size\"]),\n",
        "            nn.Conv2d(config[\"conv2d_channels\"][1], config[\"conv2d_channels\"][2], 3),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(config[\"pool_size\"]),\n",
        "            nn.Flatten()\n",
        "        )\n",
        "\n",
        "        with torch.no_grad():\n",
        "            dummy = torch.rand(1, 1, config[\"img_size\"], config[\"img_size\"])\n",
        "            features = self.backbone(dummy).shape[1]\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(features, config[\"linear_hidden\"]),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(config[\"linear_hidden\"], 19)\n",
        "        )\n",
        "\n",
        "        self.train_acc = torchmetrics.Accuracy(task='multiclass', num_classes=19)\n",
        "        self.val_acc = torchmetrics.Accuracy(task='multiclass', num_classes=19)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B,1,H,W) for 2D\n",
        "        if x.dim() == 5:\n",
        "          x = x.squeeze(1)  # Remove temporal dim if present\n",
        "        return self.classifier(self.backbone(x))\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        y_hat = self(x)\n",
        "        loss = F.cross_entropy(y_hat, y)\n",
        "        self.train_acc(y_hat, y)\n",
        "        self.log_dict({\n",
        "            \"train_loss\": loss,\n",
        "            \"train_acc\": self.train_acc\n",
        "        }, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        y_hat = self(x)\n",
        "        loss = F.cross_entropy(y_hat, y)\n",
        "        self.val_acc(y_hat, y)\n",
        "        self.log_dict({\n",
        "            \"val_loss\": loss,\n",
        "            \"val_acc\": self.val_acc\n",
        "        }, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return Adam(self.parameters(), lr=self.config[\"learning_rate\"])\n",
        "\n",
        "class CNN3D(pl.LightningModule):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "        # Modified architecture with proper dimension handling\n",
        "        self.conv3d = nn.Sequential(\n",
        "            # First conv with padding to maintain size\n",
        "            nn.Conv3d(1, config[\"conv3d_channels\"][1],\n",
        "                     kernel_size=(1, 3, 3),  # Only spatial conv first\n",
        "                     padding=(0, 1, 1)),  # Pad spatial dimensions only\n",
        "            nn.ReLU(),\n",
        "\n",
        "            # Temporal convolution after spatial features\n",
        "            nn.Conv3d(config[\"conv3d_channels\"][1], config[\"conv3d_channels\"][1],\n",
        "                     kernel_size=(3, 1, 1),  # Only temporal conv\n",
        "                     padding=(1, 0, 0)),  # Pad temporal dimension only\n",
        "            nn.ReLU(),\n",
        "\n",
        "            # Pool only spatial dimensions\n",
        "            nn.MaxPool3d((1, 2, 2)),  # Only reduce H,W\n",
        "\n",
        "            # Second conv block\n",
        "            nn.Conv3d(config[\"conv3d_channels\"][1], config[\"conv3d_channels\"][2],\n",
        "                     kernel_size=(1, 3, 3), padding=(0, 1, 1)),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool3d((1, 2, 2)),  # Only reduce H,W\n",
        "\n",
        "            # Final adaptive pooling\n",
        "            nn.AdaptiveAvgPool3d((None, 1, 1))  # Reduce spatial to 1x1\n",
        "        )\n",
        "\n",
        "        # Calculate features dynamically\n",
        "        with torch.no_grad():\n",
        "            dummy = torch.rand(1, 1, config[\"sample_frames\"][\"3D\"],\n",
        "                             config[\"img_size\"], config[\"img_size\"])\n",
        "            features = self.conv3d(dummy).numel()\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(features, config[\"linear_hidden\"]),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(config[\"linear_hidden\"], 19)\n",
        "        )\n",
        "\n",
        "        # Metrics\n",
        "        self.train_acc = torchmetrics.Accuracy(task='multiclass', num_classes=19)\n",
        "        self.val_acc = torchmetrics.Accuracy(task='multiclass', num_classes=19)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B,T,C,H,W)\n",
        "        x = x.permute(0, 2, 1, 3, 4)  # (B,C,T,H,W)\n",
        "        return self.classifier(self.conv3d(x))\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        y_hat = self(x)\n",
        "        loss = F.cross_entropy(y_hat, y)\n",
        "        self.train_acc(y_hat, y)\n",
        "        self.log_dict({\n",
        "            \"train_loss\": loss,\n",
        "            \"train_acc\": self.train_acc\n",
        "        }, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        y_hat = self(x)\n",
        "        loss = F.cross_entropy(y_hat, y)\n",
        "        self.val_acc(y_hat, y)\n",
        "        self.log_dict({\n",
        "            \"val_loss\": loss,\n",
        "            \"val_acc\": self.val_acc\n",
        "        }, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return Adam(self.parameters(), lr=self.config[\"learning_rate\"])\n",
        "\n",
        "class HybridCNN(pl.LightningModule):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "        # Spatial processing\n",
        "        self.spatial = nn.Sequential(\n",
        "            nn.Conv2d(1, config[\"conv2d_channels\"][1], 3),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(config[\"pool_size\"]),\n",
        "            nn.Conv2d(config[\"conv2d_channels\"][1], config[\"conv2d_channels\"][2], 3),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(config[\"pool_size\"]),\n",
        "            nn.Flatten()\n",
        "        )\n",
        "\n",
        "        # Temporal processing\n",
        "        with torch.no_grad():\n",
        "            dummy = torch.rand(1, 1, config[\"img_size\"], config[\"img_size\"])\n",
        "            spatial_features = self.spatial(dummy).shape[1]\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=spatial_features,\n",
        "            hidden_size=config[\"lstm_hidden\"],\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Linear(config[\"lstm_hidden\"], 19)\n",
        "\n",
        "        # Metrics\n",
        "        self.train_acc = torchmetrics.Accuracy(task='multiclass', num_classes=19)\n",
        "        self.val_acc = torchmetrics.Accuracy(task='multiclass', num_classes=19)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B,T,C,H,W)\n",
        "        batch_size, timesteps = x.shape[:2]\n",
        "        x = x.view(batch_size * timesteps, *x.shape[2:])  # (B*T,C,H,W)\n",
        "        spatial = self.spatial(x)  # (B*T, D)\n",
        "        spatial = spatial.view(batch_size, timesteps, -1)  # (B,T,D)\n",
        "\n",
        "        lstm_out, _ = self.lstm(spatial)  # (B,T,H)\n",
        "        return self.classifier(lstm_out[:,-1,:])  # Last timestep\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        y_hat = self(x)\n",
        "        loss = F.cross_entropy(y_hat, y)\n",
        "        self.train_acc(y_hat, y)\n",
        "        self.log_dict({\n",
        "            \"train_loss\": loss,\n",
        "            \"train_acc\": self.train_acc\n",
        "        }, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        y_hat = self(x)\n",
        "        loss = F.cross_entropy(y_hat, y)\n",
        "        self.val_acc(y_hat, y)\n",
        "        self.log_dict({\n",
        "            \"val_loss\": loss,\n",
        "            \"val_acc\": self.val_acc\n",
        "        }, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return Adam(self.parameters(), lr=self.config[\"learning_rate\"])\n",
        "\n",
        "# Comparison Function\n",
        "def compare_models():\n",
        "    task = Task.init(project_name=\"MovingMNIST-Comparison\",\n",
        "                    task_name=\"Architecture Comparison\")\n",
        "    task.connect(config)\n",
        "\n",
        "    results = {}\n",
        "    architectures = {\n",
        "        \"2D\": CNN2D,\n",
        "        \"3D\": CNN3D,\n",
        "        \"2D+1D\": HybridCNN\n",
        "    }\n",
        "\n",
        "    # Create base dataset\n",
        "    train_dataset = MovingMNIST(train=True,\n",
        "                              img_size=config[\"img_size\"],\n",
        "                              total_frames=config[\"total_frames\"])\n",
        "    val_dataset = MovingMNIST(train=False,\n",
        "                            img_size=config[\"img_size\"],\n",
        "                            total_frames=config[\"total_frames\"])\n",
        "\n",
        "    for model_name, model_class in architectures.items():\n",
        "        print(f\"\\n=== Training {model_name} Model ===\")\n",
        "\n",
        "        # Create frame-sampled datasets\n",
        "        train_sampler = FrameSampler(train_dataset, model_name,\n",
        "                                   config[\"sample_frames\"][model_name])\n",
        "        val_sampler = FrameSampler(val_dataset, model_name,\n",
        "                                 config[\"sample_frames\"][model_name])\n",
        "\n",
        "        train_loader = DataLoader(train_sampler,\n",
        "                                 batch_size=config[\"batch_size\"],\n",
        "                                 shuffle=True)\n",
        "        val_loader = DataLoader(val_sampler,\n",
        "                               batch_size=config[\"batch_size\"])\n",
        "\n",
        "        # Initialize model\n",
        "        model = model_class(config)\n",
        "\n",
        "        # Callbacks\n",
        "        checkpoint = ModelCheckpoint(\n",
        "            dirpath=config[\"checkpoint_dir\"],\n",
        "            filename=f\"{model_name}-best\",\n",
        "            monitor=\"val_loss\",\n",
        "            mode=\"min\"\n",
        "        )\n",
        "\n",
        "        trainer = pl.Trainer(\n",
        "            max_epochs=config[\"max_epochs\"],\n",
        "            accelerator=config[\"accelerator\"],\n",
        "            logger=CSVLogger(save_dir=f\"{config['log_dir']}/{model_name}\"),\n",
        "            callbacks=[checkpoint, EarlyStopping(monitor=\"val_loss\",\n",
        "                                               patience=config[\"early_stop_patience\"])]\n",
        "        )\n",
        "\n",
        "        # Train\n",
        "        trainer.fit(model, train_loader, val_loader)\n",
        "\n",
        "        # Store results\n",
        "        best_path = checkpoint.best_model_path\n",
        "        if best_path:\n",
        "            model = model_class.load_from_checkpoint(best_path)\n",
        "            val_results = trainer.validate(model, val_loader)[0]\n",
        "            results[model_name] = {\n",
        "                \"val_loss\": val_results[\"val_loss\"],\n",
        "                \"val_acc\": val_results[\"val_acc\"],\n",
        "                \"params\": sum(p.numel() for p in model.parameters())\n",
        "            }\n",
        "\n",
        "            # Visualize predictions\n",
        "            sample_batch = next(iter(val_loader))\n",
        "            visualize_predictions(sample_batch, model, model_name)\n",
        "\n",
        "    # Print comparison table\n",
        "    print(\"\\n=== Final Comparison ===\")\n",
        "    print(f\"{'Model':<10} {'Val Loss':<10} {'Val Acc':<10} {'Params':<10}\")\n",
        "    for name, res in results.items():\n",
        "        print(f\"{name:<10} {res['val_loss']:.4f}     {res['val_acc']:.4f}     {res['params']:,}\")\n",
        "\n",
        "    task.close()\n",
        "    return results\n",
        "\n",
        "# Visualization function\n",
        "def visualize_predictions(batch, model, model_name):\n",
        "    x, y = batch\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        preds = model(x).argmax(dim=1)\n",
        "\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.suptitle(f\"{model_name} Predictions\", fontsize=14)\n",
        "\n",
        "    for i in range(3):  # Show first 3 samples\n",
        "        plt.subplot(1, 3, i+1)\n",
        "\n",
        "        if len(x.shape) == 5:  # 3D or Hybrid - show middle frame\n",
        "            frame_idx = x.shape[1] // 2\n",
        "            plt.imshow(x[i, frame_idx, 0].cpu(), cmap='gray')\n",
        "        else:  # 2D - show single frame\n",
        "            plt.imshow(x[i, 0].cpu(), cmap='gray')\n",
        "\n",
        "        plt.title(f\"True: {y[i].item()}\\nPred: {preds[i].item()}\")\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Run comparison\n",
        "if __name__ == \"__main__\":\n",
        "    compare_models()"
      ],
      "metadata": {
        "id": "ZXnmx1UgYRml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EdidkvEHkYQr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}